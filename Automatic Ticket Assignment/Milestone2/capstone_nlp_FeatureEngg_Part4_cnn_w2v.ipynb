{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "capstone-nlp-FeatureEngg-Part4-cnn-w2v.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7KiCG86kU123"
      },
      "source": [
        "import pandas as pd  \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyN6jDatU13A",
        "outputId": "dabfca2a-5f26-459f-8984-782540fc411b"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "F96lEnr0Wamc",
        "outputId": "346276c9-8178-4815-a709-170abf75b733"
      },
      "source": [
        "my_df = pd.read_csv('drive/MyDrive/datasets/pre_data_dl_aug2.csv')\r\n",
        "#my_df = pd.read_csv(csv,index_col=0)\r\n",
        "my_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Combined Description Cleaned</th>\n",
              "      <th>Assignment group</th>\n",
              "      <th>LabelEncodings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>login issue user manager name checked the name...</td>\n",
              "      <td>GRP_0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>outlook received from hello team my are not in...</td>\n",
              "      <td>GRP_0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>cannot log in to received from hi i cannot on ...</td>\n",
              "      <td>GRP_0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>unable to access tool page</td>\n",
              "      <td>GRP_0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>error</td>\n",
              "      <td>GRP_0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... LabelEncodings\n",
              "0           0  ...              0\n",
              "1           1  ...              0\n",
              "2           2  ...              0\n",
              "3           3  ...              0\n",
              "4           4  ...              0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGKrP1pUXquK",
        "outputId": "d448b7a6-bf58-43b8-8333-4cfa16c5b9b8"
      },
      "source": [
        "df2 = my_df\r\n",
        "my_df['Assignment group by number'] = my_df['Assignment group'].str[4:]\r\n",
        "my_df['Assignment group by number'] = my_df['Assignment group by number'].astype(int)\r\n",
        "my_df.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 31221 entries, 0 to 31220\n",
            "Data columns (total 5 columns):\n",
            " #   Column                        Non-Null Count  Dtype \n",
            "---  ------                        --------------  ----- \n",
            " 0   Unnamed: 0                    31221 non-null  int64 \n",
            " 1   Combined Description Cleaned  31221 non-null  object\n",
            " 2   Assignment group              31221 non-null  object\n",
            " 3   LabelEncodings                31221 non-null  int64 \n",
            " 4   Assignment group by number    31221 non-null  int64 \n",
            "dtypes: int64(3), object(2)\n",
            "memory usage: 1.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuEE-z87U13B",
        "outputId": "d21210c3-3487-449e-8ddb-94ea49b77e5c"
      },
      "source": [
        "my_df.dropna(inplace=True)\n",
        "my_df.reset_index(drop=True,inplace=True)\n",
        "my_df.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 31221 entries, 0 to 31220\n",
            "Data columns (total 5 columns):\n",
            " #   Column                        Non-Null Count  Dtype \n",
            "---  ------                        --------------  ----- \n",
            " 0   Unnamed: 0                    31221 non-null  int64 \n",
            " 1   Combined Description Cleaned  31221 non-null  object\n",
            " 2   Assignment group              31221 non-null  object\n",
            " 3   LabelEncodings                31221 non-null  int64 \n",
            " 4   Assignment group by number    31221 non-null  int64 \n",
            "dtypes: int64(3), object(2)\n",
            "memory usage: 1.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4Jx1zNUeU13C"
      },
      "source": [
        "x = my_df['Combined Description Cleaned']\n",
        "y = my_df['Assignment group by number']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg2trRmXWlWY"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "y = tf.keras.utils.to_categorical(y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "z5swzZqEU13C"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "SEED = 2000\n",
        "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.20, random_state=SEED)\n",
        "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "UP3MBhl6U13D"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import multiprocessing\n",
        "from sklearn import utils"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "P_7Q_ofBU13D"
      },
      "source": [
        "def labelize_tickets_ug(tickets,label):\n",
        "    result = []\n",
        "    prefix = label\n",
        "    for i, t in zip(tickets.index, tickets):\n",
        "        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
        "    return result"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "FEVA1zJUU13D"
      },
      "source": [
        "all_x = pd.concat([x_train,x_validation,x_test])\n",
        "all_x_w2v = labelize_tickets_ug(all_x, 'all')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUG5zchMU13D",
        "outputId": "3cf71b98-06be-448e-fc76-738f8b82df46"
      },
      "source": [
        "cores = multiprocessing.cpu_count()\n",
        "model_ug_cbow = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
        "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31221/31221 [00:00<00:00, 2219159.20it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r51jjEVNU13G",
        "outputId": "9d39ef16-fb9b-40a2-ca7d-8738ebf9773e"
      },
      "source": [
        "%%time\n",
        "for epoch in range(30):\n",
        "    model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
        "    model_ug_cbow.alpha -= 0.002\n",
        "    model_ug_cbow.min_alpha = model_ug_cbow.alpha"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31221/31221 [00:00<00:00, 2279935.32it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2655439.94it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2364237.11it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2498623.62it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2428965.08it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2416950.26it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2623046.80it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2714447.27it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2772081.65it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2705138.93it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2710177.68it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2736398.81it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2359763.67it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2725407.20it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2591793.47it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2578981.51it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2407795.48it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2635983.03it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2259245.11it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2596366.98it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2509012.21it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2653287.78it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2725293.76it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2751925.30it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2565390.64it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2465969.25it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2580353.61it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2600181.99it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2717376.33it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2672892.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 38 s, sys: 389 ms, total: 38.4 s\n",
            "Wall time: 12 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI-fbMyoU13G",
        "outputId": "b312557c-2985-4160-e393-b5eccd36dc13"
      },
      "source": [
        "model_ug_sg = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
        "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31221/31221 [00:00<00:00, 2148664.62it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqjoGH3aU13G",
        "outputId": "9cb41a3f-eac6-4dbe-d4d3-232db92fb553"
      },
      "source": [
        "%%time\n",
        "for epoch in range(30):\n",
        "    model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
        "    model_ug_sg.alpha -= 0.002\n",
        "    model_ug_sg.min_alpha = model_ug_sg.alpha"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 31221/31221 [00:00<00:00, 2260180.97it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2608313.22it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2611486.22it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2628100.53it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2687925.72it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2308064.81it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2608521.05it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2577103.60it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2641033.52it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2517114.51it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2670821.24it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2727621.18it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2653664.16it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2590819.19it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2666524.78it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2644019.73it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2731091.29it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2391918.56it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2666036.18it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2477258.57it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2746442.22it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2636407.59it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2573204.27it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2685169.89it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2496765.66it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2762665.93it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2449043.67it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2654686.29it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2661376.42it/s]\n",
            "100%|██████████| 31221/31221 [00:00<00:00, 2595903.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 16s, sys: 324 ms, total: 1min 16s\n",
            "Wall time: 20.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IuHeRXbGU13H"
      },
      "source": [
        "model_ug_cbow.save('drive\\MyDrive\\grams\\w2v_model_ug_cbow.word2vec')\n",
        "model_ug_sg.save('drive\\MyDrive\\grams\\w2v_model_ug_sg.word2vec')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j_XoBcrU13H"
      },
      "source": [
        "# Preparation for Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr5xhOQQU13H"
      },
      "source": [
        "In the last post, I have aggregated the word vectors of each word in a ticket, either summation or calculating mean to get one vector representation of each ticket. However, in order to feed to a CNN, we have to not only feed each word vector to the model, but also in a sequence which matches the original ticket desc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Z5QmM-U13I"
      },
      "source": [
        "For example, let's say we have a sentence as below.\n",
        "\n",
        "\"I want password reset\"\n",
        "\n",
        "And let's assume that we have a 2-dimensional vector representation of each word as follows:\n",
        "\n",
        "I: [0.3, 0.5]\n",
        "want: [1.2, 0.8]\n",
        "password: [0.4, 1.3]\n",
        "\n",
        "With the above sentence, the dimension of the vector we have for the whole sentence is 3 X 2 (3: number of words, 2: number of vector dimension)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWP4zb-WU13I"
      },
      "source": [
        "But there is one more thing we need to consider. A neural network model will expect all the data to have the same dimension, but in case of different sentences, they will have different lengths. This can be handled with padding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_dKiOdlU13I"
      },
      "source": [
        "The first sentence had 3X2 dimension vectors, but the second sentence has 4X2 dimension vector. Our neural network won't accept these as inputs. By padding the inputs, we decide the maximum length of words in a sentence, then zero pads the rest, if the input length is shorter than the designated length. In the case where it exceeds the maximum length, then it will also truncate either from the beginning or from the end. For example, let's say we decide our maximum length to be 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMlWqFPVU13J"
      },
      "source": [
        "Then by padding, the first sentence will have 2 more 2-dimensional vectors of all zeros at the start or the end (you can decide this by passing an argument), and the second sentence will have 1 more 2-dimensional vector of zeros at the beginning or the end. Now we have 2 same dimensional (5X2) vectors for each sentence, and we can finally feed this to a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cJpkOv0U13J"
      },
      "source": [
        "Let's first load the Word2Vec models to extract word vectors from. I have saved the Word2Vec models I trained in the previous post, and can easily be loaded with \"KeyedVectors\" function in Gensim. I have two different Word2Vec models, one with CBOW (Continuous Bag Of Words) model, and the other with a skip-gram model. I won't go into detail of how CBOW and skip-gram differs, but you can refer to my previous post if you want to know a bit more in detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "kcItmvO8U13J"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "model_ug_cbow = KeyedVectors.load('drive\\MyDrive\\grams\\w2v_model_ug_cbow.word2vec')\n",
        "model_ug_sg = KeyedVectors.load('drive\\MyDrive\\grams\\w2v_model_ug_sg.word2vec')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x9fSZqSU13K",
        "outputId": "347b11d8-9711-4feb-8c9f-81e978ff58a7"
      },
      "source": [
        "len(model_ug_cbow.wv.vocab.keys())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3548"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKfk0oKwU13K"
      },
      "source": [
        "By running below code block, I am constructing a sort of dictionary I can extract the word vectors from. Since I have two different Word2Vec models, below \"embedding_index\" will have concatenated vectors of the two models. For each model, I have 100 dimension vector representation of the words, and by concatenating each word will have 200 dimension vector representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5vX7G5EU13K",
        "outputId": "9b6c1198-86b2-414e-db40-95fc524e7d31"
      },
      "source": [
        "embeddings_index = {}\n",
        "for w in model_ug_cbow.wv.vocab.keys():\n",
        "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3548 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEJX2pUEU13L"
      },
      "source": [
        "Now we have our reference to word vectors ready, but we still haven't prepared data to be in the format I have explained at the start of the post. Keras' 'Tokenizer' will split each word in a sentence,  then we can call 'texts_to_sequences' method to get the sequential representation of each sentence. We also need to pass 'num_words' which is a number of vocabularies you want to use, and this will be applied when you call 'texts_to_sequences' method. This might be a bit counter-intuitive since if you check the length of all the word index, it will not be the number of words you defined, but the actual screening process happens when you call 'texts_to_sequences' method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dF2ZCp4eU13L"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100000)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "sequences = tokenizer.texts_to_sequences(x_train)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqoUFkJLU13L",
        "outputId": "594d78f4-4b22-46a3-ecc5-da60b07cb3ae"
      },
      "source": [
        "len(tokenizer.word_index)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHRMcb28U13M"
      },
      "source": [
        "Below are the first five entries of the original train data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lMW-HabU13M",
        "outputId": "37ac992f-f1ea-4fed-b83e-6215835be037"
      },
      "source": [
        "for x in x_train[:5]:\n",
        "    print( x)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metal cutting technology company hi team i have the attached quote however it is not up showing correctly see attached can you please fix this as soon possible\n",
            "office is not defined for area when we new order got communicate that incomplete sale try enter right i not add the print screen in attached\n",
            "unable to access engineering tool received from dear sir following error message shown request support resolve\n",
            "unable to share screen language explorer customer number telephone summary am my on\n",
            "trigger making need rate th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlmIHugrU13M"
      },
      "source": [
        "And the same data prepared as sequential dats is as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H77Z2Df_U13M",
        "outputId": "05305fda-4a82-4c98-ffa8-62ed0fb09fd7"
      },
      "source": [
        "sequences[:5]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2048,\n",
              "  1529,\n",
              "  914,\n",
              "  35,\n",
              "  84,\n",
              "  71,\n",
              "  16,\n",
              "  20,\n",
              "  2,\n",
              "  89,\n",
              "  466,\n",
              "  340,\n",
              "  13,\n",
              "  5,\n",
              "  7,\n",
              "  56,\n",
              "  236,\n",
              "  317,\n",
              "  64,\n",
              "  89,\n",
              "  31,\n",
              "  23,\n",
              "  6,\n",
              "  264,\n",
              "  17,\n",
              "  39,\n",
              "  561,\n",
              "  171],\n",
              " [197,\n",
              "  5,\n",
              "  7,\n",
              "  932,\n",
              "  10,\n",
              "  352,\n",
              "  33,\n",
              "  40,\n",
              "  83,\n",
              "  101,\n",
              "  274,\n",
              "  1798,\n",
              "  34,\n",
              "  1321,\n",
              "  1663,\n",
              "  276,\n",
              "  196,\n",
              "  484,\n",
              "  16,\n",
              "  7,\n",
              "  244,\n",
              "  2,\n",
              "  169,\n",
              "  140,\n",
              "  4,\n",
              "  89],\n",
              " [60, 1, 36, 120, 22, 12, 3, 145, 418, 138, 29, 57, 476, 135, 219, 260],\n",
              " [60, 1, 737, 140, 215, 240, 102, 105, 245, 308, 38, 50, 9],\n",
              " [1710, 478, 42, 915, 322]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O3g2-n2U13N"
      },
      "source": [
        "Each word is represented as a number, and we can see that the number of words in each sentence is matching the length of numbers in the \"sequences\". We can later make connections of which word each number represents. But we still didn't pad our data, so each sentence has varying length. Let's deal with this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "COThDV1fU13N"
      },
      "source": [
        "length = []\n",
        "for x in x_train:\n",
        "    length.append(len(x.split()))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWDVdJSoU13N",
        "outputId": "8df4bcdc-9810-4baa-b9ca-a79028f3fd35"
      },
      "source": [
        "max(length)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "191"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZL_9duSU13N"
      },
      "source": [
        "The maximum number of words in a sentence within the training data is 191. Let's decide the maximum length to be a bit longer than this, let's say 200."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wwdknTfU13N",
        "outputId": "fc42dd6a-a86f-4ceb-cf99-833195d8c1df"
      },
      "source": [
        "x_train_seq = pad_sequences(sequences, maxlen=200)\n",
        "print('Shape of data tensor:', x_train_seq.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (24976, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Yi4_HshU13O",
        "outputId": "f1612d62-9774-41b7-948f-0995ee75da65"
      },
      "source": [
        "x_train_seq[:5]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0, 2048, 1529,  914,   35,\n",
              "          84,   71,   16,   20,    2,   89,  466,  340,   13,    5,    7,\n",
              "          56,  236,  317,   64,   89,   31,   23,    6,  264,   17,   39,\n",
              "         561,  171],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,  197,    5,\n",
              "           7,  932,   10,  352,   33,   40,   83,  101,  274, 1798,   34,\n",
              "        1321, 1663,  276,  196,  484,   16,    7,  244,    2,  169,  140,\n",
              "           4,   89],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,   60,    1,   36,\n",
              "         120,   22,   12,    3,  145,  418,  138,   29,   57,  476,  135,\n",
              "         219,  260],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          60,    1,  737,  140,  215,  240,  102,  105,  245,  308,   38,\n",
              "          50,    9],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0, 1710,  478,   42,\n",
              "         915,  322]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmUPiTPqU13O"
      },
      "source": [
        "As you can see from the padded sequences, all the data now transformed to have the same length of 45, and by default, Keras zero-pads at the beginning, if a sentence length is shorter than the maximum length. If you want to know more in detail, please check the Keras documentation on sequence preprocessing. https://keras.io/preprocessing/sequence/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ktOhcc6KU13O"
      },
      "source": [
        "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
        "x_val_seq = pad_sequences(sequences_val, maxlen=200)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc_Z4RScU13O"
      },
      "source": [
        "There's still one more thing left to do before we can feed the sequential text data to a model. When we transformed a sentence into a sequence, each word is represented by an integer number. Actually, these numbers are where each word is stored in the tokenizer's word index. Keeping this in mind, let's build a matrix of these word vectors, but this time we will use the word index number so that our model can refer to the corresponding vector when fed with integer sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6mEC23sU13P"
      },
      "source": [
        "Below, I am defining the number of words to be 100,000. This means I will only care about 100,000 most frequent words in the training set. If I don't limit the number of words, the total number of vocabulary will be more than 200,000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "BqwHGMkDU13P"
      },
      "source": [
        "num_words = 100000\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9nVCW6sU13P"
      },
      "source": [
        "As a sanity check, if the embedding matrix has been generated properly. In the above, when I saw the first five entries of the training set, the first entry was \"hate you\", and the sequential representation of this was [137, 6]. Let's see if 6th embedding matrix is as same as vectors for the word 'you'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sK7ebz8ZhH5",
        "outputId": "419e507a-15f4-4f28-bb8c-e774275555a2"
      },
      "source": [
        "embeddings_index.get('network')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.4111273e-01,  2.9155019e-01, -1.1284587e-01,  2.2359337e-01,\n",
              "        6.0058411e-02, -4.9900252e-02,  8.5473888e-02, -7.8683376e-01,\n",
              "        8.6925071e-01,  2.1847136e+00, -9.3664986e-01,  7.3335814e-01,\n",
              "       -6.8616986e-02, -8.7594610e-01,  1.1092713e-01, -1.2582666e+00,\n",
              "        1.2278801e+00,  1.2878528e+00, -9.8261803e-01, -7.2262418e-01,\n",
              "        1.1262896e+00,  9.7218889e-01, -8.3947498e-01, -6.3291377e-01,\n",
              "       -4.9763948e-01,  5.3166255e-02, -9.0042315e-03, -2.1348071e+00,\n",
              "       -1.5790586e-01,  9.1814911e-01, -2.8721866e-01, -1.0304505e+00,\n",
              "       -3.6228693e-01, -3.4313101e-01,  8.9838028e-01, -3.1134722e-01,\n",
              "        8.2395124e-01, -3.6602399e-01,  8.8751358e-01, -6.9557905e-02,\n",
              "       -2.3045862e-01, -1.9827213e+00,  5.2359575e-01, -1.7394167e+00,\n",
              "        3.3287084e-01,  3.7697442e-02, -3.7102136e-01,  1.8132128e+00,\n",
              "       -1.3087019e-01, -1.1318239e+00,  7.6131320e-01,  1.1042281e+00,\n",
              "        2.3107922e+00, -8.0040866e-01,  9.7324580e-02, -6.1856121e-01,\n",
              "       -1.0442916e+00,  9.5791191e-02, -4.0579689e-01,  6.4438981e-01,\n",
              "        5.5636275e-01,  1.4529211e-02, -1.2574714e+00,  3.1071660e-01,\n",
              "       -4.7035301e-01, -1.1002653e-01, -1.9197249e+00,  1.0191275e+00,\n",
              "       -6.8996876e-01,  5.0513476e-01, -9.3815410e-01,  1.2709042e-01,\n",
              "        6.8883485e-01,  1.4953889e+00, -1.5713260e-01,  1.3692358e+00,\n",
              "        1.0568380e+00,  7.3271587e-02,  6.5501064e-01, -4.9749336e-01,\n",
              "        1.7789316e+00,  5.7362837e-01,  1.7563044e+00,  1.3985208e-01,\n",
              "        3.4267822e-01,  4.0076026e-01, -3.8480851e-01, -1.0851266e+00,\n",
              "       -1.5952419e+00, -9.7116850e-02, -2.7238634e-01,  4.0377459e-01,\n",
              "       -7.5472391e-01, -1.6648157e-01,  1.0725148e+00, -2.3068576e+00,\n",
              "        5.2028346e-01, -8.3445293e-01,  1.2173096e+00, -9.7408760e-01,\n",
              "        4.3597180e-01, -3.6612245e-01,  2.5762585e-01,  5.9493417e-01,\n",
              "        1.3433740e-01,  4.0345535e-01, -5.5677019e-02, -2.3530415e-01,\n",
              "        2.8176117e-01,  3.5009143e-01,  1.7162670e-01,  1.9270039e-01,\n",
              "        2.8633279e-01, -5.2419508e-01,  3.9128253e-01, -3.0725059e-01,\n",
              "        1.6824870e-01, -1.9342909e-03, -3.9637658e-01,  5.8352917e-01,\n",
              "        9.0961441e-02,  1.5014224e-02,  1.4965732e-01, -9.4418067e-01,\n",
              "        1.3229258e-01,  6.6026539e-01, -3.2921821e-01, -5.3098100e-01,\n",
              "        1.0331578e-01,  2.4311787e-01, -2.7896157e-01, -1.9132788e-01,\n",
              "        3.0532771e-01,  3.3053815e-01,  2.1808478e-01, -6.4156669e-01,\n",
              "        7.4115831e-01, -5.3186548e-01,  7.4748778e-01,  2.8327668e-01,\n",
              "       -6.3415885e-02, -3.9450353e-01, -6.6613543e-01, -8.7265372e-01,\n",
              "        6.4824533e-01, -2.2695056e-01, -4.7966334e-01,  2.0528245e-01,\n",
              "        1.4318249e-01, -3.6747959e-02, -1.7037557e-01,  5.0623339e-01,\n",
              "       -3.0323103e-02, -3.5710022e-01, -1.2926095e+00,  2.4025969e-01,\n",
              "        4.1882802e-02,  3.2251734e-01, -4.2178661e-02,  4.4234684e-01,\n",
              "        1.9255918e-01, -4.6764734e-01,  5.3419644e-01, -1.5048774e-01,\n",
              "       -6.6860253e-01, -4.9818221e-01, -1.3121659e-01,  1.0553851e+00,\n",
              "       -5.6082749e-01,  3.2706174e-01,  2.7694502e-01,  3.9498636e-01,\n",
              "        3.8910770e-01, -1.8866946e-01, -6.4254165e-02,  7.9018362e-02,\n",
              "        4.6131092e-01, -3.6645183e-01,  5.0027078e-01,  3.1861421e-01,\n",
              "       -1.8241130e-01, -1.1756031e-03,  5.0050962e-01,  1.2892696e-01,\n",
              "        6.2320113e-01, -3.3633739e-01, -3.6396289e-01, -7.6748945e-02,\n",
              "       -4.9289280e-01, -5.4100430e-01, -1.6095865e-01, -1.7391486e-01,\n",
              "        8.4267519e-03,  5.3004521e-01,  7.0725232e-01, -2.6113021e-01,\n",
              "       -5.2604276e-01, -8.0024880e-01, -1.8580747e-01, -4.1247073e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed7hGXdbU13P",
        "outputId": "4cbecabe-9658-4e45-f074-7e6b2198dcf7"
      },
      "source": [
        "np.array_equal(embedding_matrix[6] ,embeddings_index.get('network'))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOhUNHXEU13Q"
      },
      "source": [
        "Now we are ready with the data preparation. Before we jump into CNN, I would like to test one more thing (sorry for the delay). When we feed this sequential vector representation of data, we will use Embedding layer in Keras. With Embedding layer, I can either pass pre-defined embedding, which I prepared as 'embedding_matrix' above, or Embedding layer itself can learn word embeddings as the whole model trains. And another possibility is we can still feed the pre-defined embedding but make it trainable so that it will update the values of vectors as the model trains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMM-OtuBU13Q"
      },
      "source": [
        "In order to check which method performs better, I defined a simple shallow neural network one hidden layer. For this model structure, I will not try to refine models by tweaking parameters, since the main purpose of this post is to implement CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOOBfNl-U13Q"
      },
      "source": [
        "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j80fsEAcU13Q"
      },
      "source": [
        "seed = 7\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNBvgXuXU13Q",
        "outputId": "24438f0b-37c0-4d0b-c373-d19e9ffbce86"
      },
      "source": [
        "model_ptw2v = Sequential()\n",
        "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=200, trainable=False)\n",
        "model_ptw2v.add(e)\n",
        "model_ptw2v.add(Flatten())\n",
        "model_ptw2v.add(Dense(256, activation='relu'))\n",
        "model_ptw2v.add(Dense(74, activation='softmax'))\n",
        "model_ptw2v.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_ptw2v.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "781/781 - 4s - loss: 0.9522 - accuracy: 0.7717 - val_loss: 0.3015 - val_accuracy: 0.9074\n",
            "Epoch 2/5\n",
            "781/781 - 3s - loss: 0.2124 - accuracy: 0.9400 - val_loss: 0.2621 - val_accuracy: 0.9190\n",
            "Epoch 3/5\n",
            "781/781 - 3s - loss: 0.1496 - accuracy: 0.9534 - val_loss: 0.2496 - val_accuracy: 0.9270\n",
            "Epoch 4/5\n",
            "781/781 - 3s - loss: 0.1431 - accuracy: 0.9563 - val_loss: 0.2521 - val_accuracy: 0.9183\n",
            "Epoch 5/5\n",
            "781/781 - 3s - loss: 0.1694 - accuracy: 0.9508 - val_loss: 0.6520 - val_accuracy: 0.8725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdb4061b208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ritbyO6gU13R",
        "outputId": "6ea3d05a-5002-42fb-d956-0b97d346950e"
      },
      "source": [
        "model_ptw2v = Sequential()\n",
        "e = Embedding(100000, 200, input_length=200)\n",
        "model_ptw2v.add(e)\n",
        "model_ptw2v.add(Flatten())\n",
        "model_ptw2v.add(Dense(256, activation='relu'))\n",
        "model_ptw2v.add(Dense(74, activation='softmax'))\n",
        "model_ptw2v.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_ptw2v.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "781/781 - 92s - loss: 1.7356 - accuracy: 0.5766 - val_loss: 0.5821 - val_accuracy: 0.8523\n",
            "Epoch 2/5\n",
            "781/781 - 91s - loss: 0.3180 - accuracy: 0.9137 - val_loss: 0.2827 - val_accuracy: 0.9215\n",
            "Epoch 3/5\n",
            "781/781 - 91s - loss: 0.1713 - accuracy: 0.9487 - val_loss: 0.2607 - val_accuracy: 0.9209\n",
            "Epoch 4/5\n",
            "781/781 - 91s - loss: 0.1359 - accuracy: 0.9572 - val_loss: 0.2610 - val_accuracy: 0.9209\n",
            "Epoch 5/5\n",
            "781/781 - 91s - loss: 0.1185 - accuracy: 0.9600 - val_loss: 0.2448 - val_accuracy: 0.9244\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdb402f9198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB3cpZlZU13R",
        "outputId": "fb846941-8df8-4cfa-e87b-31f98a643471"
      },
      "source": [
        "model_ptw2v = Sequential()\n",
        "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=200, trainable=True)\n",
        "model_ptw2v.add(e)\n",
        "model_ptw2v.add(Flatten())\n",
        "model_ptw2v.add(Dense(256, activation='relu'))\n",
        "model_ptw2v.add(Dense(74, activation='softmax'))\n",
        "model_ptw2v.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_ptw2v.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "781/781 - 90s - loss: 0.9620 - accuracy: 0.7673 - val_loss: 0.3163 - val_accuracy: 0.9049\n",
            "Epoch 2/5\n",
            "781/781 - 88s - loss: 0.2068 - accuracy: 0.9395 - val_loss: 0.2504 - val_accuracy: 0.9196\n",
            "Epoch 3/5\n",
            "781/781 - 85s - loss: 0.1508 - accuracy: 0.9528 - val_loss: 0.2462 - val_accuracy: 0.9215\n",
            "Epoch 4/5\n",
            "781/781 - 85s - loss: 0.1387 - accuracy: 0.9578 - val_loss: 0.2318 - val_accuracy: 0.9308\n",
            "Epoch 5/5\n",
            "781/781 - 85s - loss: 0.1252 - accuracy: 0.9591 - val_loss: 0.3202 - val_accuracy: 0.9145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdb4013a2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luievVJxU13R"
      },
      "source": [
        "As a result, the best validation accuracy is from the third method (fine-tune pre-trained Word2Vec) at 82.22%. The best training accuracy is the second method (learn word embedding from scratch) at 90.52%. Using pre-trained Word2Vec without updating its vector values showed the lowest accuracy both in training and validation. However, what's interesting is that in terms of training set accuracy, fine-tuning pre-trained word vectors couldn't outperform the word embeddings learned from scratch through the embedding layer. Before I tried the above three methods, my first guess was that if I fine-tune the pre-trained word vectors, it would give me the best training accuracy.\n",
        "\n",
        "Feeding pre-trained word vectors for an embedding layer to update is like providing the first initialisation guideline to the embedding layer so that it can learn more efficiently the task-specific word vectors. But the result is somewhat counterintuitive, and in this case, it turns out that it is better to force the embedding layer to learn from scratch.\n",
        "\n",
        "But premature generalization is a dangerous step to take. For this reason, I will compare three methods again in the context of CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFNjGkNMU13R"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "J2Ds0axsU13S"
      },
      "source": [
        "from keras.layers import Conv1D, GlobalMaxPooling1D"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEQ872MFU13T",
        "outputId": "44f8d9a7-7e96-4683-bf4d-537067032499"
      },
      "source": [
        "structure_test = Sequential()\n",
        "e = Embedding(100000, 200, input_length=200)\n",
        "structure_test.add(e)\n",
        "structure_test.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
        "structure_test.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 200, 200)          20000000  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 199, 100)          40100     \n",
            "=================================================================\n",
            "Total params: 20,040,100\n",
            "Trainable params: 20,040,100\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4Ey5PD6U13T"
      },
      "source": [
        "Now if we add Global Max Pooling layer, then the pooling layer will extract the maximum value from each filter, and the output dimension will be a just 1-dimensional vector with length as same as the number of filters we applied. This can be directly passed on to a dense layer without flattening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5u3ary9U13T",
        "outputId": "182516e3-64f1-4938-c58e-7256d57be959"
      },
      "source": [
        "structure_test = Sequential()\n",
        "e = Embedding(100000, 200, input_length=200)\n",
        "structure_test.add(e)\n",
        "structure_test.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
        "structure_test.add(GlobalMaxPooling1D())\n",
        "structure_test.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 200, 200)          20000000  \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 199, 100)          40100     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 100)               0         \n",
            "=================================================================\n",
            "Total params: 20,040,100\n",
            "Trainable params: 20,040,100\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkQy-YDzU13T"
      },
      "source": [
        "Now, let's define a simple CNN going through bigrams on a ticket. The output from global max pooling layer will be fed to a fully connected layer, then finally the output layer. Again I will try three different inputs, static word vectors extracted from Word2Vec, word embedding being learned from scratch with embedding layer, Word2Vec word vectors being updated through training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF9RLqqMU13T",
        "outputId": "8e8ce21b-7319-44be-a0ec-6d3ce2899246"
      },
      "source": [
        "model_cnn_01 = Sequential()\n",
        "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=200, trainable=False)\n",
        "model_cnn_01.add(e)\n",
        "model_cnn_01.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
        "model_cnn_01.add(GlobalMaxPooling1D())\n",
        "model_cnn_01.add(Dense(256, activation='relu'))\n",
        "model_cnn_01.add(Dense(74, activation='softmax'))\n",
        "model_cnn_01.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_cnn_01.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 45) for input KerasTensor(type_spec=TensorSpec(shape=(None, 45), dtype=tf.float32, name='embedding_5_input'), name='embedding_5_input', description=\"created by layer 'embedding_5_input'\"), but it was called on an input with incompatible shape (None, 200).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 45) for input KerasTensor(type_spec=TensorSpec(shape=(None, 45), dtype=tf.float32, name='embedding_5_input'), name='embedding_5_input', description=\"created by layer 'embedding_5_input'\"), but it was called on an input with incompatible shape (None, 200).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 45) for input KerasTensor(type_spec=TensorSpec(shape=(None, 45), dtype=tf.float32, name='embedding_5_input'), name='embedding_5_input', description=\"created by layer 'embedding_5_input'\"), but it was called on an input with incompatible shape (None, 200).\n",
            "781/781 - 7s - loss: 2.0155 - accuracy: 0.4782 - val_loss: 1.2359 - val_accuracy: 0.6634\n",
            "Epoch 2/5\n",
            "781/781 - 3s - loss: 0.8579 - accuracy: 0.7569 - val_loss: 0.7455 - val_accuracy: 0.7841\n",
            "Epoch 3/5\n",
            "781/781 - 3s - loss: 0.5206 - accuracy: 0.8497 - val_loss: 0.6025 - val_accuracy: 0.8238\n",
            "Epoch 4/5\n",
            "781/781 - 3s - loss: 0.3802 - accuracy: 0.8873 - val_loss: 0.5165 - val_accuracy: 0.8527\n",
            "Epoch 5/5\n",
            "781/781 - 3s - loss: 0.3030 - accuracy: 0.9072 - val_loss: 0.5120 - val_accuracy: 0.8479\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdaf223d7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP5TPXg5U13T",
        "outputId": "0ce2866b-1b48-4054-f048-ad5a5684d9ca"
      },
      "source": [
        "model_cnn_02 = Sequential()\n",
        "e = Embedding(100000, 200, input_length=200)\n",
        "model_cnn_02.add(e)\n",
        "model_cnn_02.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
        "model_cnn_02.add(GlobalMaxPooling1D())\n",
        "model_cnn_02.add(Dense(256, activation='relu'))\n",
        "model_cnn_02.add(Dense(74, activation='softmax'))\n",
        "model_cnn_02.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_cnn_02.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "781/781 - 91s - loss: 2.1598 - accuracy: 0.4558 - val_loss: 1.1445 - val_accuracy: 0.6915\n",
            "Epoch 2/5\n",
            "781/781 - 90s - loss: 0.6566 - accuracy: 0.8241 - val_loss: 0.5242 - val_accuracy: 0.8575\n",
            "Epoch 3/5\n",
            "781/781 - 90s - loss: 0.2899 - accuracy: 0.9179 - val_loss: 0.4128 - val_accuracy: 0.8831\n",
            "Epoch 4/5\n",
            "781/781 - 90s - loss: 0.1916 - accuracy: 0.9437 - val_loss: 0.3879 - val_accuracy: 0.8905\n",
            "Epoch 5/5\n",
            "781/781 - 90s - loss: 0.1595 - accuracy: 0.9527 - val_loss: 0.3820 - val_accuracy: 0.8997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdb619c4780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HbjYcMkU13U",
        "outputId": "f34a9515-36c3-470d-9641-9075fa5dc913"
      },
      "source": [
        "model_cnn_03 = Sequential()\n",
        "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=200, trainable=True)\n",
        "model_cnn_03.add(e)\n",
        "model_cnn_03.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
        "model_cnn_03.add(GlobalMaxPooling1D())\n",
        "model_cnn_03.add(Dense(256, activation='relu'))\n",
        "model_cnn_03.add(Dense(74, activation='softmax'))\n",
        "model_cnn_03.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_cnn_03.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "781/781 - 91s - loss: 1.8500 - accuracy: 0.5227 - val_loss: 1.0158 - val_accuracy: 0.7127\n",
            "Epoch 2/5\n",
            "781/781 - 89s - loss: 0.6592 - accuracy: 0.8149 - val_loss: 0.5878 - val_accuracy: 0.8258\n",
            "Epoch 3/5\n",
            "781/781 - 89s - loss: 0.3603 - accuracy: 0.8934 - val_loss: 0.4969 - val_accuracy: 0.8568\n",
            "Epoch 4/5\n",
            "781/781 - 88s - loss: 0.2684 - accuracy: 0.9198 - val_loss: 0.4683 - val_accuracy: 0.8728\n",
            "Epoch 5/5\n",
            "781/781 - 88s - loss: 0.2152 - accuracy: 0.9332 - val_loss: 0.3956 - val_accuracy: 0.8805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdb61821748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srEvBsaYU13V",
        "outputId": "c2a11d2e-2e12-4618-9e5d-085e372db7f1"
      },
      "source": [
        "from keras.layers import Input, Dense, concatenate, Activation\n",
        "from keras.models import Model\n",
        "\n",
        "ticket_input = Input(shape=(200,), dtype='int32')\n",
        "\n",
        "ticket_encoder = Embedding(100000, 200, weights=[embedding_matrix], input_length=200, trainable=True)(ticket_input)\n",
        "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(ticket_encoder)\n",
        "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
        "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(ticket_encoder)\n",
        "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
        "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(ticket_encoder)\n",
        "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
        "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
        "\n",
        "merged = Dense(256, activation='relu')(merged)\n",
        "merged = Dropout(0.2)(merged)\n",
        "merged = Dense(74)(merged)\n",
        "output = Activation('softmax')(merged)\n",
        "model = Model(inputs=[ticket_input], outputs=[output])\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_9 (Embedding)         (None, 200, 200)     20000000    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 199, 100)     40100       embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 198, 100)     60100       embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 197, 100)     80100       embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_5 (GlobalM (None, 100)          0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_6 (GlobalM (None, 100)          0           conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_7 (GlobalM (None, 100)          0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 300)          0           global_max_pooling1d_5[0][0]     \n",
            "                                                                 global_max_pooling1d_6[0][0]     \n",
            "                                                                 global_max_pooling1d_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 256)          77056       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 256)          0           dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 74)           19018       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 74)           0           dense_15[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 20,276,374\n",
            "Trainable params: 20,276,374\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jZf6vdHU13V",
        "outputId": "763659c0-5577-4ebd-f1a4-d6d36a7c9bf4"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
        "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "model.fit(x_train_seq, y_train, batch_size=32, epochs=5,\n",
        "                     validation_data=(x_val_seq, y_validation)) #, callbacks = [checkpoint])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "781/781 [==============================] - 91s 115ms/step - loss: 2.5075 - accuracy: 0.3919 - val_loss: 0.7200 - val_accuracy: 0.7944\n",
            "Epoch 2/5\n",
            "781/781 [==============================] - 89s 115ms/step - loss: 0.6113 - accuracy: 0.8314 - val_loss: 0.3773 - val_accuracy: 0.8901\n",
            "Epoch 3/5\n",
            "781/781 [==============================] - 90s 115ms/step - loss: 0.3056 - accuracy: 0.9104 - val_loss: 0.3877 - val_accuracy: 0.8831\n",
            "Epoch 4/5\n",
            "781/781 [==============================] - 89s 114ms/step - loss: 0.2449 - accuracy: 0.9255 - val_loss: 0.3417 - val_accuracy: 0.8985\n",
            "Epoch 5/5\n",
            "781/781 [==============================] - 90s 115ms/step - loss: 0.1812 - accuracy: 0.9434 - val_loss: 0.3387 - val_accuracy: 0.9023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdb615f3780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3qfp4_KU13V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d9936c5-ed63-4b2e-fdfe-1e4c35d795f8"
      },
      "source": [
        "from keras.models import load_model\n",
        "#loaded_CNN_model = load_model('CNN_best_weights.02-0.8333.hdf5')\n",
        "#loaded_CNN_model.evaluate(x=x_val_seq, y=y_validation)\n",
        "model.evaluate(x=x_val_seq, y=y_validation)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98/98 [==============================] - 0s 4ms/step - loss: 0.3387 - accuracy: 0.9023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3387061357498169, 0.9023061990737915]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9fsxYabU13V"
      },
      "source": [
        "## Final Model Evaluation with Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMOGDyEzU13V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4916a2fa-39d8-421d-d25d-1e3422b14b4b"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tvec = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))\n",
        "tvec.fit(x_train)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0,\n",
              "                max_features=100000, min_df=1, ngram_range=(1, 3), norm='l2',\n",
              "                preprocessor=None, smooth_idf=True, stop_words=None,\n",
              "                strip_accents=None, sublinear_tf=False,\n",
              "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
              "                vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4wyIcTHU13W"
      },
      "source": [
        "x_train_tfidf = tvec.transform(x_train)\n",
        "x_test_tfidf = tvec.transform(x_test)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3mLc1W8OU13W"
      },
      "source": [
        "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
        "x_test_seq = pad_sequences(sequences_test, maxlen=200)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d__nhkQBU13W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b1fccbd-94b0-44d4-aa27-fb16aa16995c"
      },
      "source": [
        "model.evaluate(x=x_test_seq, y=y_test)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98/98 [==============================] - 0s 4ms/step - loss: 0.3215 - accuracy: 0.8985\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3215217590332031, 0.8984950184822083]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rwKqnFwcU13W"
      },
      "source": [
        "yhat_cnn = model.predict(x_test_seq)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32nmco4qe94c"
      },
      "source": [
        "# A method to train and test the model\r\n",
        "def run_dl_model(model, X_train, X_test, y_train, y_test, \r\n",
        "                       epochs=10,batch_size=128, logs_dir=\"my_logs\"):\r\n",
        "\r\n",
        "    tf.keras.backend.clear_session()\r\n",
        "    #checkpoint_every_epoch = get_checkpoint_every_epoch()\r\n",
        "    run_logdir = get_run_logdir(root_logdir=logs_dir) # e.g., './my_logs/' + run_id\r\n",
        "    #tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\r\n",
        "    #checkpoint_best_only = get_checkpoint_best_only()\r\n",
        "    early_stopping = get_early_stopping(monitor='val_loss', mode='min',patience=5)\r\n",
        "    reduce_lr = reduce_learning_rate_on_plateaue(monitor='val_loss', mode='min', patience=5)\r\n",
        "    with tf.device(\"/gpu:0\"): \r\n",
        "        history = model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=epochs, batch_size=batch_size,verbose=1,\r\n",
        "                            callbacks=[#tensorboard_cb, PrintValTrainRatioCallback(),\r\n",
        "                            #checkpoint_every_epoch,\r\n",
        "                            #checkpoint_best_only,\r\n",
        "                            reduce_lr ,\r\n",
        "                            print_lr,\r\n",
        "                            early_stopping\r\n",
        "                            ])\r\n",
        "      \r\n",
        "    return history"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz0IjLlExVCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2871fe9-3090-46e1-de83-018fd4120247"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 17.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 17.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (20.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.19.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (1.0.0)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp36-none-any.whl size=78939 sha256=9260c3e890beb88ffa1d344aaa3bf75f453c3b6bf04233ecdf84da3d49dea09f\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15358 sha256=2301ff4c0cf0959c61013f88a3055ea445a351cabc0db0170f3f72066a80d1d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKSb5qoewzx5"
      },
      "source": [
        "import os\r\n",
        "from kerastuner import RandomSearch\r\n",
        "from kerastuner.engine import hyperparameters\r\n",
        "import kerastuner as kt"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVFPDHjxwDXS"
      },
      "source": [
        "def get_evaluation_accuracy(model, X_test, y_test):\r\n",
        "    \"\"\"Test model classification accuracy\"\"\"\r\n",
        "    eval_loss, eval_acc = model.evaluate(x=X_test, y=y_test, verbose=0)\r\n",
        "    print('Evaluation Accuracy: {acc:0.3f}'.format(acc=eval_acc))\r\n",
        "    print('Evaluation Loss: {acc:0.3f}'.format(acc=eval_loss))\r\n",
        "\r\n",
        "# Function to build Deep NN\r\n",
        "def build_dnn_model(shape, nClasses, dropout=0.3):\r\n",
        "    model = Sequential()\r\n",
        "    node = 512 # number of nodes\r\n",
        "    nLayers = 4 # number of  hidden layer\r\n",
        "    model.add(Dense(node,input_dim=shape,activation='relu'))\r\n",
        "    model.add(Dropout(dropout))\r\n",
        "    for i in range(0,nLayers):\r\n",
        "        model.add(Dense(node,input_dim=node,activation='relu'))\r\n",
        "        model.add(Dropout(dropout))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "    model.add(Dense(nClasses, activation='softmax'))\r\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\r\n",
        "                  optimizer='adam',\r\n",
        "                  metrics=['accuracy'])\r\n",
        "    print(model.summary())\r\n",
        "    return model\r\n",
        "\r\n",
        "def get_optimizer():\r\n",
        "    return tf.keras.optimizers.SGD(lr=0.001, momentum=0.9)\r\n",
        "\r\n",
        "def build_cnn_model(word_index, embeddings_matrix, nclasses,dropout=0.2):\r\n",
        "    model = Sequential()\r\n",
        "    embedding_layer = Embedding(len(word_index) + 1,\r\n",
        "                                EMBEDDING_DIM,\r\n",
        "                                weights=[embeddings_matrix],\r\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\r\n",
        "                                trainable=True)\r\n",
        "  # applying a more complex convolutional approach\r\n",
        "    convs = []\r\n",
        "    filter_sizes = []\r\n",
        "    layer = 5\r\n",
        "    for fl in range(0,layer):\r\n",
        "        filter_sizes.append((fl+2))\r\n",
        "    node = 128\r\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\r\n",
        "    embedded_sequences = embedding_layer(sequence_input)\r\n",
        "    for fsz in filter_sizes:\r\n",
        "        l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\r\n",
        "        l_pool = MaxPooling1D(5)(l_conv)\r\n",
        "        #l_pool = Dropout(0.25)(l_pool)\r\n",
        "        convs.append(l_pool)\r\n",
        "    l_merge = Concatenate(axis=1)(convs)\r\n",
        "    l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\r\n",
        "    l_cov1 = Dropout(dropout)(l_cov1)\r\n",
        "    l_batch1 = BatchNormalization()(l_cov1)\r\n",
        "    l_pool1 = MaxPooling1D(5)(l_batch1)\r\n",
        "    l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\r\n",
        "    l_cov2 = Dropout(dropout)(l_cov2)\r\n",
        "    l_batch2 = BatchNormalization()(l_cov2)\r\n",
        "    l_pool2 = MaxPooling1D(30)(l_batch2)\r\n",
        "    l_flat = Flatten()(l_pool2)\r\n",
        "    l_dense = Dense(1024, activation='relu')(l_flat)\r\n",
        "    l_dense = Dropout(dropout)(l_dense)\r\n",
        "    l_dense = Dense(512, activation='relu')(l_dense)\r\n",
        "    l_dense = Dropout(dropout)(l_dense)\r\n",
        "    preds = Dense(nclasses, activation='softmax')(l_dense)\r\n",
        "    model = Model(sequence_input, preds)\r\n",
        "    \r\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\r\n",
        "                  optimizer=get_optimizer(),\r\n",
        "                  metrics=['accuracy'])\r\n",
        "    \r\n",
        "    print(model.summary())\r\n",
        "    return model\r\n",
        "\r\n",
        "# Build GRU model\r\n",
        "def build_gru_model(word_index, embeddings_matrix, nclasses,dropout=0.2):\r\n",
        "    model = Sequential()\r\n",
        "    hidden_layer = 6\r\n",
        "    gru_node = 32\r\n",
        "    \r\n",
        "    model.add(Embedding(len(word_index) + 1,\r\n",
        "                                EMBEDDING_DIM,\r\n",
        "                                weights=[embeddings_matrix],\r\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\r\n",
        "                                trainable=True))\r\n",
        "    print(gru_node)\r\n",
        "    for i in range(0,hidden_layer):\r\n",
        "        model.add(tf.compat.v1.keras.layers.CuDNNGRU(gru_node,return_sequences=True))\r\n",
        "        model.add(Dropout(dropout))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "    model.add(tf.compat.v1.keras.layers.CuDNNGRU(gru_node))\r\n",
        "    model.add(Dropout(dropout))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dense(256, activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dense(nclasses, activation='softmax'))\r\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\r\n",
        "                      optimizer=get_optimizer(),\r\n",
        "                      metrics=['accuracy'])\r\n",
        "    \r\n",
        "    print(model.summary())\r\n",
        "    return model\r\n",
        "\r\n",
        "# Build lstm model\r\n",
        "\r\n",
        "def build_lstm_model(#word_index, \r\n",
        "                     nclasses=74,\r\n",
        "                     dropout=0.2,\r\n",
        "                     hidden_layer=6,\r\n",
        "                     lstm_node = 3,\r\n",
        "                     dense_node= 256,\r\n",
        "                     loss='adam',\r\n",
        "#                     optimizer=get_optimizer(),\r\n",
        "                     accuracy='accuracy',\r\n",
        "                     activation='relu',\r\n",
        "                     output_activation='softmax'\r\n",
        "                    ):\r\n",
        "    print(embedding_matrix)\r\n",
        "    print(len(word_index) + 1)\r\n",
        "    print(EMBEDDING_DIM)\r\n",
        "    model = Sequential() \r\n",
        "    model.add(Embedding(len(word_index) + 1,\r\n",
        "                                EMBEDDING_DIM,\r\n",
        "                                weights=[embedding_matrix],\r\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\r\n",
        "                                trainable=True))\r\n",
        "    #print(gru_node)\r\n",
        "    for i in range(0,hidden_layer):\r\n",
        "        model.add(tf.compat.v1.keras.layers.CuDNNLSTM(lstm_node,return_sequences=True))\r\n",
        "        model.add(Dropout(dropout))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "    model.add(tf.compat.v1.keras.layers.CuDNNLSTM(lstm_node))\r\n",
        "    model.add(Dropout(dropout))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dense(dense_node, activation=activation ))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dense(nclasses, activation=output_activation))\r\n",
        "    model.compile(loss=loss,\r\n",
        "                      optimizer='adam',\r\n",
        "                      metrics=[accuracy])\r\n",
        "    \r\n",
        "    print(model.summary())\r\n",
        "    return model\r\n",
        "\r\n",
        "def get_model(input_shape, batch_size, len):\r\n",
        "  \"\"\"\r\n",
        "  model = Sequential([\r\n",
        "                      Flatten(input_shape=input_shape),\r\n",
        "                      Dense(128*6, activation='relu'),\r\n",
        "                      Dense(128*6, activation='relu'\r\n",
        "                            #kernel_regularizer=tf.keras.regularizers.l2(0.01)\r\n",
        "                      ),\r\n",
        "                      Dense(128*6, activation='relu'\r\n",
        "                            #kernel_regularizer=tf.keras.regularizers.l2(0.01)\r\n",
        "                      ),\r\n",
        "                      Dense(10, activation='softmax')\r\n",
        "\r\n",
        "  ])\r\n",
        "  model = Sequential([\r\n",
        "                      Flatten(input_shape=input_shape),\r\n",
        "                      #BatchNormalization(),\r\n",
        "                      Dense(128*6, kernel_initializer='he_uniform'),\r\n",
        "                      #BatchNormalization(),\r\n",
        "                      Activation('relu', ),\r\n",
        "                      Dropout(0.2),\r\n",
        "                      Dense(128*6, kernel_initializer='he_uniform'),\r\n",
        "                      #BatchNormalization(),\r\n",
        "                      Activation('relu'),\r\n",
        "                      Dropout(0.2),\r\n",
        "                      Dense(128*6, kernel_initializer='he_uniform'),\r\n",
        "                      #BatchNormalization(),\r\n",
        "                      Activation('relu'),\r\n",
        "                      Dense(10, activation='softmax')\r\n",
        "\r\n",
        "  ])\r\n",
        "  \"\"\"\r\n",
        "  #for SELU - prerequisites are initialzer=lecun_normal, input should be standardized, \r\n",
        "  \r\n",
        "  model = Sequential([\r\n",
        "                      Flatten(input_shape=input_shape),\r\n",
        "                      BatchNormalization(),\r\n",
        "                      Dense(128*6, kernel_initializer='lecun_normal'),\r\n",
        "                      BatchNormalization(),\r\n",
        "                      Activation('selu', ),\r\n",
        "                      Dropout(0.2),\r\n",
        "                      Dense(128*6, kernel_initializer='lecun_normal'),\r\n",
        "                      BatchNormalization(),\r\n",
        "                      Activation('selu'),\r\n",
        "                      Dropout(0.2),\r\n",
        "                      Dense(128*6, kernel_initializer='lecun_normal'),\r\n",
        "                      BatchNormalization(),\r\n",
        "                      Activation('selu'),\r\n",
        "                      Dense(10, activation='softmax')\r\n",
        "\r\n",
        "  ])\r\n",
        "  #s = 20 * len // batch_size # number of steps in 20 epochs (batch size = 32)\r\n",
        "  #learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\r\n",
        "  #optimizer = tf.keras.optimizers.Adam(lr=1e-4, amsgrad=True)\r\n",
        "  optimizer=tf.keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True) \r\n",
        "  model.compile(optimizer=optimizer,\r\n",
        "                loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n",
        "                metrics=['accuracy'])\r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def reduce_learning_rate_on_plateaue(patience=3,monitor='val_loss',mode='min'):\r\n",
        "    #reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.3, patience=patience,\r\n",
        "    #                                                 mode=mode, verbose=1)\r\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=monitor, \r\n",
        "                                  factor=0.1, \r\n",
        "                                  patience=10, \r\n",
        "                                  verbose=1, \r\n",
        "                                  mode=mode, \r\n",
        "                                  min_delta=0.0001, \r\n",
        "                                  cooldown=5, \r\n",
        "                                  min_lr=1e-6)\r\n",
        "    return reduce_lr\r\n",
        "\r\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\r\n",
        "              lambda epoch: 1e-3 * 10**(epoch / 10))\r\n",
        "\r\n",
        "\r\n",
        "def get_early_stopping(patience=3,monitor='val_loss',mode='min'):\r\n",
        "    \"\"\"\r\n",
        "    This function should return an EarlyStopping callback that stops training when\r\n",
        "    the validation (testing) accuracy has not improved in the last 3 epochs.\r\n",
        "    HINT: use the EarlyStopping callback with the correct 'monitor' and 'patience'\r\n",
        "    \"\"\"\r\n",
        "    return tf.keras.callbacks.EarlyStopping(\r\n",
        "        monitor=monitor,  patience=patience, verbose=1, mode=mode\r\n",
        "    )   \r\n",
        "\r\n",
        "def get_checkpoint_every_epoch( checkpoint_path = 'checkpoints_every_epoch/checkpoint_{epoch:03d}'):\r\n",
        "    \"\"\"\r\n",
        "    This function should return a ModelCheckpoint object that:\r\n",
        "    - saves the weights only at the end of every epoch\r\n",
        "    - saves into a directory called 'checkpoints_every_epoch' inside the current working directory\r\n",
        "    - generates filenames in that directory like 'checkpoint_XXX' where\r\n",
        "      XXX is the epoch number formatted to have three digits, e.g. 001, 002, 003, etc.\r\n",
        "    \"\"\"\r\n",
        "    checkpoint_path = checkpoint_path\r\n",
        "    return ModelCheckpoint(filepath=checkpoint_path,\r\n",
        "                             save_freq='epoch', #if save_freq is an integer like 1000, save every 1000 samples \r\n",
        "                             save_weights_only=True,verbose=1)   \r\n",
        "\r\n",
        "def get_checkpoint_best_only(monitor='val_accuracy',\r\n",
        "                             checkpoint_best_path = 'checkpoints_best_only/checkpoint'):\r\n",
        "    \"\"\"\r\n",
        "    This function should return a ModelCheckpoint object that:\r\n",
        "    - saves only the weights that generate the highest validation (testing) accuracy\r\n",
        "    - saves into a directory called 'checkpoints_best_only' inside the current working directory\r\n",
        "    - generates a file called 'checkpoints_best_only/checkpoint' \r\n",
        "    \"\"\"\r\n",
        "    checkpoint_best_path = checkpoint_best_path\r\n",
        "    return ModelCheckpoint(filepath=checkpoint_best_path,\r\n",
        "                                  save_weights_only=True,\r\n",
        "                                  save_freq='epoch',\r\n",
        "                                  save_best_only=False,\r\n",
        "                                  verbose=1,\r\n",
        "                                  monitor=monitor)    \r\n",
        "    \r\n",
        "def get_run_logdir(root_logdir=\"my_logs\"):\r\n",
        "    import time\r\n",
        "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\r\n",
        "    return os.path.join(root_logdir, run_id)\r\n",
        "\r\n",
        "#run_logdir = get_run_logdir() # e.g., './my_logs/' + run_id\r\n",
        "\r\n",
        "#tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\r\n",
        "\r\n",
        "class LossAndMetricCallback(tf.keras.callbacks.Callback):\r\n",
        "\r\n",
        "    # Print the loss after every second batch in the training set\r\n",
        "    def on_train_batch_end(self, batch, logs=None):\r\n",
        "        if batch %2 ==0:\r\n",
        "            print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\r\n",
        "    \r\n",
        "    # Print the loss after each batch in the test set\r\n",
        "    def on_test_batch_end(self, batch, logs=None):\r\n",
        "        print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\r\n",
        "\r\n",
        "    # Print the loss and mean absolute error after each epoch\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        print('Epoch {}: Average loss is {:7.2f}, mean absolute error is {:7.2f}.'.format(epoch, logs['loss'], logs['mae']))\r\n",
        "    \r\n",
        "    # Notify the user when prediction has finished on each batch\r\n",
        "    def on_predict_batch_end(self,batch, logs=None):\r\n",
        "        print(\"Finished prediction on batch {}!\".format(batch))\r\n",
        "\r\n",
        "class PrintValTrainRatioCallback(tf.keras.callbacks.Callback):\r\n",
        "    def on_epoch_end(self, epoch, logs):\r\n",
        "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\r\n",
        "\r\n",
        "class lr_print_cb(tf.keras.callbacks.Callback):\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        lr = self.model.optimizer.lr\r\n",
        "        #decay = self.model.optimizer.decay\r\n",
        "        #iterations = self.model.optimizer.iterations\r\n",
        "        #lr_with_decay = lr / (1. + decay * K.cast(iterations, K.dtype(decay)))\r\n",
        "        print(\"Learning Rate = \", lr)\r\n",
        "        \r\n",
        "print_lr = lr_print_cb()\r\n",
        "\r\n",
        "test_logdir = get_run_logdir()\r\n",
        "writer = tf.summary.create_file_writer(test_logdir)\r\n",
        "with writer.as_default():\r\n",
        "    for step in range(1, 1000 + 1):\r\n",
        "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\r\n",
        "        data = (np.random.randn(100) + 2) * step / 100 # some random data\r\n",
        "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\r\n",
        "        images = np.random.rand(2, 32, 32, 3) # random 32×32 RGB images\r\n",
        "        tf.summary.image(\"my_images\", images * step / 1000, step=step)\r\n",
        "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step**2)]\r\n",
        "        tf.summary.text(\"my_text\", texts, step=step)\r\n",
        "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\r\n",
        "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\r\n",
        "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)\r\n",
        "        \r\n",
        "#Function to plot accuracy\r\n",
        "def plot_accuracy(history):\r\n",
        "  try:\r\n",
        "      plt.plot(history.history['accuracy'])\r\n",
        "      plt.plot(history.history['val_accuracy'])\r\n",
        "  except KeyError:\r\n",
        "      plt.plot(history.history['acc'])\r\n",
        "      plt.plot(history.history['val_acc'])\r\n",
        "  plt.title('Accuracy vs. epochs')\r\n",
        "  plt.ylabel('Accuracy')\r\n",
        "  plt.xlabel('Epoch')\r\n",
        "  plt.legend(['Training', 'Validation'], loc='lower right')\r\n",
        "  plt.show() \r\n",
        "\r\n",
        "#Function to plot loss\r\n",
        "def plot_loss(history, scale=1):\r\n",
        "  try:\r\n",
        "      plt.plot(history.history['loss'])\r\n",
        "      plt.plot( [x / scale for x in history.history['val_loss']] )\r\n",
        "  except KeyError:\r\n",
        "      plt.plot(history.history['loss'])\r\n",
        "      plt.plot([x / scale for x in history.history['val_loss']] )\r\n",
        "  plt.title('Loss vs. epochs')\r\n",
        "  plt.ylabel('Loss')\r\n",
        "  plt.xlabel('Epoch')\r\n",
        "  plt.legend(['Training', 'Validation'], loc='upper right')\r\n",
        "  plt.show() \r\n",
        "\r\n",
        "def plot_to_image(figure):\r\n",
        "  \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\r\n",
        "  returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\r\n",
        "  # Save the plot to a PNG in memory.\r\n",
        "  buf = io.BytesIO()\r\n",
        "  plt.savefig(buf, format='png')\r\n",
        "  # Closing the figure prevents it from being displayed directly inside\r\n",
        "  # the notebook.\r\n",
        "  plt.close(figure)\r\n",
        "  buf.seek(0)\r\n",
        "  # Convert PNG buffer to TF image\r\n",
        "  image = tf.image.decode_png(buf.getvalue(), channels=4)\r\n",
        "  # Add the batch dimension\r\n",
        "  image = tf.expand_dims(image, 0)\r\n",
        "  return image\r\n",
        "\r\n",
        "#Function to plot the confusion matrix that will be called at the end of each epoch\r\n",
        "#The confusion matrix will be visible in the Tensor Board  \r\n",
        "class_names=['0','1','2','3','4','5','6','7','8','9']\r\n",
        "def plot_confusion_matrix(cm, class_names):\r\n",
        "  \"\"\"\r\n",
        "  Returns a matplotlib figure containing the plotted confusion matrix.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    cm (array, shape = [n, n]): a confusion matrix of integer classes\r\n",
        "    class_names (array, shape = [n]): String names of the integer classes\r\n",
        "  \"\"\"\r\n",
        "  figure = plt.figure(figsize=(8, 8))\r\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\r\n",
        "  plt.title(\"Confusion matrix\")\r\n",
        "  plt.colorbar()\r\n",
        "  tick_marks = np.arange(len(class_names))\r\n",
        "  plt.xticks(tick_marks, class_names, rotation=45)\r\n",
        "  plt.yticks(tick_marks, class_names)\r\n",
        "\r\n",
        "  # Normalize the confusion matrix.\r\n",
        "  cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\r\n",
        "\r\n",
        "  # Use white text if squares are dark; otherwise black.\r\n",
        "  threshold = cm.max() / 2.\r\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
        "    color = \"white\" if cm[i, j] > threshold else \"black\"\r\n",
        "    plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\r\n",
        "\r\n",
        "  plt.tight_layout()\r\n",
        "  plt.ylabel('True label')\r\n",
        "  plt.xlabel('Predicted label')\r\n",
        "  return figure\r\n",
        "\r\n",
        "def log_confusion_matrix(epoch, logs):\r\n",
        "  # Use the model to predict the values from the validation dataset.\r\n",
        "  test_pred_raw = model1.predict(test_images)\r\n",
        "  test_pred = np.argmax(test_pred_raw, axis=1)\r\n",
        "\r\n",
        "  # Calculate the confusion matrix.\r\n",
        "  cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)\r\n",
        "  # Log the confusion matrix as an image summary.\r\n",
        "  figure = plot_confusion_matrix(cm, class_names=class_names)\r\n",
        "  cm_image = plot_to_image(figure)\r\n",
        "\r\n",
        "  # Log the confusion matrix as an image summary.\r\n",
        "  with file_writer_cm.as_default():\r\n",
        "    tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\r\n",
        "\r\n",
        "# Define the per-epoch callback.\r\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)\r\n",
        "\r\n",
        "def print_values(history):\r\n",
        "  print(\"Mean Accuracy for the validation dataset: \")\r\n",
        "  print(np.array(history.history['val_accuracy']).mean())\r\n",
        "  print(\"Mean Loss for the validation dataset: \")\r\n",
        "  print(np.array(history.history['val_loss']).mean())\r\n",
        "  print(\"Mean Accuracy for the training dataset: \")\r\n",
        "  print(np.array(history.history['accuracy']).mean())\r\n",
        "  print(\"Mean Loss for the training dataset: \")\r\n",
        "  print(np.array(history.history['loss']).mean())\r\n",
        "\r\n",
        "def print_results(model, X_train, X_test, y_train, y_test):\r\n",
        "    y_pred = model.predict(X_test)\r\n",
        "    y_pred = np.argmax(y_pred, axis=1)\r\n",
        "    y_train_pred = model.predict(X_train)\r\n",
        "    y_train_pred = np.argmax(y_train_pred, axis=1)\r\n",
        "    \r\n",
        "    print('Prediction Model:', model)\r\n",
        "    print('-'*80)\r\n",
        "    print('Training accuracy: %.2f%%' % (accuracy_score(y_train,y_train_pred) * 100))\r\n",
        "    print('Testing accuracy: %.2f%%' % (accuracy_score(y_test, y_pred) * 100))\r\n",
        "    print('-'*80)\r\n",
        "    #print('Confusion matrix:\\n %s' % (confusion_matrix(y_test, y_pred)))\r\n",
        "    cm = confusion_matrix(y_test, y_pred)\r\n",
        "    \r\n",
        "    \r\n",
        "    cm_df = pd.DataFrame(cm)\r\n",
        "    plt.figure(figsize=(20,10))  \r\n",
        "    sns.heatmap(cm_df, annot=True)\r\n",
        "    print('-'*80)\r\n",
        "    print('Classification report:\\n %s' % (classification_report(y_test, y_pred)))\r\n",
        "\r\n",
        "def print_scores(model,X_test,y_test):\r\n",
        "    y_pred = model.predict(X_test)\r\n",
        "    y_pred = np.argmax(y_pred, axis=1)\r\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\r\n",
        "    print('Precision: %f' % precision)\r\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\r\n",
        "    print('Recall: %f' % recall)\r\n",
        "    # f1: 2 tp / (2 tp + fp + fn)\r\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\r\n",
        "    print('F1 score: %f' % f1)\r\n",
        "    print('-'*80)\r\n",
        "\r\n",
        "    \r\n",
        "def print_classification_report(model,X_test,y_test):\r\n",
        "    y_pred = model.predict(X_test)\r\n",
        "    y_pred = np.argmax(y_pred, axis=1)\r\n",
        "    print('Classification report:\\n %s' % (classification_report(y_test, y_pred)))\r\n",
        "\r\n",
        "    \r\n",
        "def print_confusion_matrix(model,X_test,y_test):\r\n",
        "    y_pred = model.predict(X_test)\r\n",
        "    y_pred = np.argmax(y_pred, axis=1)\r\n",
        "    cm = confusion_matrix(y_test, y_pred)  \r\n",
        "    cm_df = pd.DataFrame(cm)\r\n",
        "    plt.figure(figsize=(20,10))  \r\n",
        "    sns.heatmap(cm_df, annot=True)\r\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D0gFxXcwIvF"
      },
      "source": [
        "#%%script false --no-raise-error\r\n",
        "from tensorboard.plugins.hparams import api as hp2\r\n",
        "def build_model(hp):\r\n",
        "\r\n",
        "\r\n",
        "    ticket_input = Input(shape=(200,), dtype='int32')\r\n",
        "\r\n",
        "    ticket_encoder = Embedding(100000, 200, weights=[embedding_matrix], input_length=200, trainable=True)(ticket_input)\r\n",
        "    bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(ticket_encoder)\r\n",
        "    bigram_branch = GlobalMaxPooling1D()(bigram_branch)\r\n",
        "    trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(ticket_encoder)\r\n",
        "    trigram_branch = GlobalMaxPooling1D()(trigram_branch)\r\n",
        "    fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(ticket_encoder)\r\n",
        "    fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\r\n",
        "    merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\r\n",
        "\r\n",
        "    merged = Dense(\r\n",
        "        units=hp.Int('dense_1_units', min_value=32, max_value=512, step=16),\r\n",
        "        activation='relu')(merged)  \r\n",
        "    merged = Dropout(\r\n",
        "                hp.Choice('dropout_1', values = [0.1,0.2,0.3,0.4,0.5]),\r\n",
        "            )(merged)\r\n",
        "    merged = Dense(74)(merged)\r\n",
        "    output = Activation('softmax')(merged)\r\n",
        "    model = Model(inputs=[ticket_input], outputs=[output])\r\n",
        "    model.compile(loss='categorical_crossentropy',\r\n",
        "                    optimizer='adam',\r\n",
        "                    metrics=['accuracy'])\r\n",
        "    model.summary()\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "    #word_index, embeddings_matrix, nclasses = get_params()\r\n",
        "    model = Sequential()\r\n",
        "    hidden_layer = 6\r\n",
        "    #gru_node = 32\r\n",
        "    dropout=0.2\r\n",
        "    #HP_DROPOUT = hp2.HParam('dropout', hp2.RealInterval(0.1, 0.2))\r\n",
        "\r\n",
        "    model.add(Embedding(len(word_index) + 1,\r\n",
        "                                EMBEDDING_DIM,\r\n",
        "                                weights=[embedding_matrix],\r\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\r\n",
        "                                trainable=True))\r\n",
        "    #print(gru_node)\r\n",
        "    for i in range(0,hidden_layer):\r\n",
        "        model.add(tf.keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(\r\n",
        "            units=hp.Int('lstm_node_1_units', min_value=16, max_value=256, step=16),\r\n",
        "            #units=hp.Choice('lstm_node_1_units', values = [32,128,256]),\r\n",
        "           return_sequences=True\r\n",
        "        )))\r\n",
        "        model.add(\r\n",
        "            Dropout(\r\n",
        "                hp.Choice('dropout_1', values = [0.1,0.2,0.3,0.4,0.5]),\r\n",
        "            ))\r\n",
        "        model.add(BatchNormalization())\r\n",
        "    model.add(tf.keras.layers.Bidirectional(\r\n",
        "        #tf.compat.v1.keras.layers.CuDNNGRU(gru_node)\r\n",
        "        tf.compat.v1.keras.layers.CuDNNLSTM(\r\n",
        "        units=hp.Int('lstm_node_2_units', min_value=16, max_value=256, step=16)\r\n",
        "            #units=hp.Choice('lstm_node_2_units', values = [32,128,256])\r\n",
        "    )))\r\n",
        "    model.add(\r\n",
        "        Dropout(\r\n",
        "                hp.Choice('dropout_2', values = [0.1,0.2,0.3,0.4,0.5]),   \r\n",
        "        ))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(\r\n",
        "        #Dense(256, activation='relu')\r\n",
        "        Dense(units=hp.Int('dense_1_units', min_value=32, max_value=512, step=16),\r\n",
        "              #units=hp.Choice('dense_1_units', values = [32,128,256]),\r\n",
        "        activation='relu'\r\n",
        "    ))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dense(74, activation='softmax'))\r\n",
        "    #lr = hp.Choice('learning_rate', values=[0.001, 1e-2, 1e-3, 1e-4])\r\n",
        "    #momentum = hp.Choice('momentum', values=[0.0, 0.2, 0.4, 0.6, 0.8, 0.9])\r\n",
        "    optimizer=tf.keras.optimizers.SGD(hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]),\r\n",
        "                                      hp.Choice('momentum', values=[0.0, 0.2, 0.4, 0.6, 0.8, 0.9]))\r\n",
        "    model.compile(optimizer=optimizer,\r\n",
        "                loss = 'sparse_categorical_crossentropy',\r\n",
        "                metrics=['accuracy']\r\n",
        "                )\r\n",
        "    return model"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iBMGDv_wO07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030e1a64-5e7f-4d70-bddc-10c2e0ceb581"
      },
      "source": [
        "#%%script false --no-raise-error\r\n",
        "tuner_search = RandomSearch(build_model,\r\n",
        "                            objective='val_accuracy',\r\n",
        "                            max_trials=10, \r\n",
        "                            directory='outputs', \r\n",
        "                            project_name='TicketAssignment'\r\n",
        "                            )"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 200, 200)     20000000    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 199, 100)     40100       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 198, 100)     60100       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 197, 100)     80100       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 100)          0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 300)          0           global_max_pooling1d[0][0]       \n",
            "                                                                 global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           9632        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 32)           0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 74)           2442        dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 74)           0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 20,192,374\n",
            "Trainable params: 20,192,374\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gza0yYWWwSwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f43187-24a1-4285-d1a8-7774da0c7587"
      },
      "source": [
        "#%%script false --no-raise-error\r\n",
        "#logs_dir=\"logs_model_CNN\"\r\n",
        "#run_logdir = get_run_logdir(root_logdir=logs_dir) # e.g., './my_logs/' + run_id\r\n",
        "#tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\r\n",
        "checkpoint_best_only = get_checkpoint_best_only()\r\n",
        "early_stopping = get_early_stopping(monitor='val_loss', mode='min',patience=10)\r\n",
        "reduce_lr = reduce_learning_rate_on_plateaue(monitor='val_loss', mode='min', patience=10)\r\n",
        "\r\n",
        "\r\n",
        "tuner_search.search(x_train_seq, y_train,epochs=10, batch_size=16, \r\n",
        "                   validation_data=(x_val_seq, y_validation), \r\n",
        "                   verbose=1,\r\n",
        "                   callbacks=[#tensorboard_cb,\r\n",
        "                            #PrintValTrainRatioCallback(),\r\n",
        "                            #checkpoint_every_epoch,\r\n",
        "                            #checkpoint_best_only,\r\n",
        "                            print_lr,\r\n",
        "                            reduce_lr , early_stopping\r\n",
        "                            ])\r\n",
        "\r\n",
        "#history = run_dl_model(model_GRU, X_train_Glove, X_test_Glove, y_train, y_test,\r\n",
        "#                   epochs=3, batch_size=32,logs_dir=\"logs_model_CNN\")#    with tf.device(\"/gpu:0\"): \r\n",
        "#        history = model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=epochs, batch_size=batch_size,verbose=1,\r\n",
        "#                            callbacks=[tensorboard_cb, PrintValTrainRatioCallback(),\r\n",
        "#                            #checkpoint_every_epoch,\r\n",
        "#                            checkpoint_best_only,\r\n",
        "#                            reduce_lr , early_stopping\r\n",
        "#                            ])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 27m 58s]\n",
            "val_accuracy: 0.9061499238014221\n",
            "\n",
            "Best val_accuracy So Far: 0.9167200326919556\n",
            "Total elapsed time: 04h 49m 52s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4jdr0kfEyJn"
      },
      "source": [
        "#while True:pass"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i7rnGSVwV8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61182977-62f9-4086-b38e-0b86f7d4b191"
      },
      "source": [
        " #%%script false --no-raise-error\r\n",
        "hyperparams = tuner_search.get_best_hyperparameters(num_trials = 5)[0]\r\n",
        "hyperparams"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<kerastuner.engine.hyperparameters.HyperParameters at 0x7fdaa8762358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIYG-Kz4wZxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45dd8c2d-3767-4502-b568-08f4ccfca90d"
      },
      "source": [
        "#%%script false --no-raise-error\r\n",
        "best_model = tuner_search.get_best_models(num_models=1)[0]\r\n",
        "loss, mse = best_model.evaluate(x_val_seq, y_validation)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 200, 200)     20000000    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 199, 100)     40100       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 198, 100)     60100       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 197, 100)     80100       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 100)          0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 300)          0           global_max_pooling1d[0][0]       \n",
            "                                                                 global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 272)          81872       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 272)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 74)           20202       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 74)           0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 20,282,374\n",
            "Trainable params: 20,282,374\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "98/98 [==============================] - 1s 4ms/step - loss: 0.3529 - accuracy: 0.9178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q3L797HwdiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38dbaec7-5c10-4ee5-bd96-48ef0fb6fb0b"
      },
      "source": [
        "#%%script false --no-raise-error\r\n",
        "history = run_dl_model(best_model, x_train_seq, x_val_seq, y_train,  \r\n",
        "                   y_validation, epochs=100,batch_size=16,logs_dir=\"logs_model_CNN\")#  \r\n",
        "\r\n",
        "  #                 run_dl_model(model, X_train, X_test, y_train, y_test, \r\n",
        "   #                    epochs=10,batch_size=128, logs_dir=\"my_logs\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1561/1561 [==============================] - 169s 108ms/step - loss: 0.2487 - accuracy: 0.9199 - val_loss: 0.3775 - val_accuracy: 0.9061\n",
            "Learning Rate =  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
            "Epoch 2/100\n",
            "1561/1561 [==============================] - 168s 107ms/step - loss: 0.2547 - accuracy: 0.9234 - val_loss: 0.4267 - val_accuracy: 0.9017\n",
            "Learning Rate =  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
            "Epoch 3/100\n",
            "1561/1561 [==============================] - 168s 108ms/step - loss: 0.2379 - accuracy: 0.9281 - val_loss: 0.4342 - val_accuracy: 0.9081\n",
            "Learning Rate =  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
            "Epoch 4/100\n",
            "1561/1561 [==============================] - 168s 108ms/step - loss: 0.2356 - accuracy: 0.9278 - val_loss: 0.4028 - val_accuracy: 0.9116\n",
            "Learning Rate =  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
            "Epoch 5/100\n",
            "1561/1561 [==============================] - 168s 108ms/step - loss: 0.2315 - accuracy: 0.9305 - val_loss: 0.4783 - val_accuracy: 0.9042\n",
            "Learning Rate =  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
            "Epoch 6/100\n",
            "1561/1561 [==============================] - 168s 108ms/step - loss: 0.2179 - accuracy: 0.9329 - val_loss: 0.4472 - val_accuracy: 0.9084\n",
            "Learning Rate =  <tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op5jFeYwPCQL"
      },
      "source": [
        "while True:pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrt80ldRYW0i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}