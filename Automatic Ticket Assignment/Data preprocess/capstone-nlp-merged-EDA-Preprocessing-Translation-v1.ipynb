{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud\n",
    "#!pip install langdetect\n",
    "#!pip install googletrans\n",
    "#!pip install textblob\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en_core_web_md\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#!pip install -U spacy-lookups-data\n",
    "#!pip install langid\n",
    "#!pip install google_trans_new\n",
    "#!pip uninstall googletrans\n",
    "#!pip install autocorrect\n",
    "#!pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from langdetect import detect\n",
    "from itertools import cycle\n",
    "#import googletrans\n",
    "#from googletrans import Translator\n",
    "from google_trans_new import google_translator \n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "#from textblob import TextBlob\n",
    "#from textblob.translate import NotTranslated\n",
    "import random\n",
    "import operator\n",
    "import math\n",
    "import tqdm\n",
    "import time\n",
    "import spacy\n",
    "import json\n",
    "import langid\n",
    "from bs4 import BeautifulSoup\n",
    "from string import digits\n",
    "\n",
    "from autocorrect import Speller\n",
    "from ftfy import fix_encoding, fix_text, fix_text_segment, badness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section below contains  Useful Functions \n",
    "- As we find new functions, we will create them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"didnt\": \"did not\",\n",
    "\"doesnt\": \"does not\",\n",
    "\"thats\": \"that is\",\n",
    "\"wasnt\": \"was not\",\n",
    "\"weren\": \"were not\",\n",
    "\"theyre\": \"there\",\n",
    "\"dont\": \"do not\",\n",
    "\"cant\": \"cannot\",\n",
    "\"arent\": \"are not\",\n",
    "\"whats\": \"what is\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "  def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "  return contractions_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_word_cloud(column):\n",
    "    \n",
    "    comment_words = ' '\n",
    "    stopwords = set(STOPWORDS)\n",
    "\n",
    "    # iterate through the csv file \n",
    "    for val in column: \n",
    "\n",
    "        # typecaste each val to string \n",
    "        val = str(val) \n",
    "\n",
    "        # split the value \n",
    "        tokens = val.split() \n",
    "\n",
    "        # Converts each token into lowercase \n",
    "        for i in range(len(tokens)): \n",
    "            tokens[i] = tokens[i].lower() \n",
    "\n",
    "        for words in tokens: \n",
    "            comment_words = comment_words + words + ' '\n",
    "\n",
    "\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='white', \n",
    "                    stopwords = stopwords, \n",
    "                    min_font_size = 10).generate(comment_words) \n",
    "    \n",
    "    return wordcloud\n",
    "\n",
    "def removeString(data, regex):\n",
    "    return data.str.lower().str.replace(regex.lower(), ' ')\n",
    "\n",
    "def preprocess(dataset, columnsToPreprocess, regexList):\n",
    "    for column in columnsToPreprocess:\n",
    "        for regex in regexList:\n",
    "            dataset[column] = removeString(dataset[column], regex)\n",
    "            dataset[column] = dataset[column].apply(clean_step2)\n",
    "    return dataset\n",
    "\n",
    "def clean_step2(text):\n",
    "#1)remove html tags    \n",
    "   soup=BeautifulSoup(text,\"html.parser\")\n",
    "   text=soup.get_text(separator=\"\")\n",
    "    \n",
    "#2) Remove non-ASCII characters\n",
    "   encoded_string = text.encode(\"ascii\", \"ignore\")\n",
    "   text= encoded_string.decode()\n",
    "   \n",
    "#3)lower case    \n",
    "   text=text.lower()\n",
    "   text = ' '.join([w for w in text.split()])\n",
    "\n",
    "#4)remove punctuation       \n",
    "#   text = re.sub(r'[^\\w\\s]', '',text) \n",
    "   \n",
    "#5)remove whitespaces\n",
    "   text=\" \".join(text.split())\n",
    " \n",
    "#6)remove  digits  \n",
    "   remove_digits = str.maketrans('', '', digits) \n",
    "   text = text.translate(remove_digits) \n",
    "    \n",
    "#7)remove emails   \n",
    "#   text = re.sub(r'\\S*@\\S*\\s?', '', text)\n",
    "   \n",
    "#8)remove hyperlinks\n",
    "#   text = re.sub(r'https?:\\/\\/.*\\/\\w*','', text)\n",
    "   \n",
    "#9)remove other characters   \n",
    "   text=text.replace(\"_\",\" \")\n",
    "   \n",
    "   return text   \n",
    "\n",
    "def getRegexList():\n",
    "    '''\n",
    "    Adding regex list as per the given data set to flush off the unnecessary text\n",
    "    \n",
    "    '''\n",
    "    regexList = []\n",
    "    regexList += ['From:(.*)\\r\\n']  # from line\n",
    "    regexList += ['Sent:(.*)\\r\\n']  # sent to line\n",
    "    regexList += ['received from:(.*)\\r\\n']  # received data line\n",
    "    regexList += ['received']  # received data line\n",
    "    regexList += ['To:(.*)\\r\\n']  # to line\n",
    "    regexList += ['CC:(.*)\\r\\n']  # cc line\n",
    "    regexList += ['(.*)infection']  # footer\n",
    "    regexList += ['\\[cid:(.*)]']  # images cid\n",
    "    regexList += ['https?:[^\\]\\n\\r]+']  # https & http\n",
    "    regexList += ['Subject:']\n",
    "    regexList += ['[\\w\\d\\-\\_\\.]+@[\\w\\d\\-\\_\\.]+']  # emails are not required\n",
    "    regexList += ['[0-9][\\-0–90-9 ]+']  # phones are not required\n",
    "    regexList += ['[0-9]']  # numbers not needed\n",
    "    regexList += ['[^a-zA-z 0-9]+']  # anything that is not a letter\n",
    "    regexList += ['[\\r\\n]']  # \\r\\n\n",
    "    regexList += [' [a-zA-Z] ']  # single letters makes no sense\n",
    "    regexList += [' [a-zA-Z][a-zA-Z] ']  # two-letter words makes no sense\n",
    "    regexList += [\"  \"]  # double spaces\n",
    "    \n",
    "    regexList += ['^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4})$']\n",
    "    regexList += ['[\\w\\d\\-\\_\\.]+ @ [\\w\\d\\-\\_\\.]+']\n",
    "    regexList += ['Subject:']\n",
    "    regexList += ['[^a-zA-Z]']\n",
    "\n",
    "    return regexList\n",
    "\n",
    "\n",
    "def lemmatize(stringlist):\n",
    "    processed_all_documents = list()\n",
    "\n",
    "    for desc in stringlist:\n",
    "        word_tokens = word_tokenize(desc) \n",
    "    \n",
    "        filtered_sentence = [] \n",
    "\n",
    "        # Removing Stopwords\n",
    "        for w in word_tokens: \n",
    "            if w not in stop_words: \n",
    "                filtered_sentence.append(w) \n",
    "    \n",
    "        # Lemmetization\n",
    "        lemma_word = []\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        for w in filtered_sentence:\n",
    "            word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
    "            word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "            word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "            lemma_word.append(word3)\n",
    "        words = ' '.join(lemma_word)\n",
    "        processed_all_documents.append(words) \n",
    "    return processed_all_documents\n",
    "\n",
    "\n",
    "# Write a function to apply to the dataset to detect garbage data\n",
    "def detect_garbage(text):\n",
    "    if not badness.sequence_weirdness(text):\n",
    "        # nothing weird, should be okay\n",
    "        return True\n",
    "    try:\n",
    "        text.encode('sloppy-windows-1252')\n",
    "    except UnicodeEncodeError:\n",
    "        # Not CP-1252 encodable, probably fine\n",
    "        return True\n",
    "    else:\n",
    "        # Encodable as CP-1252, Mojibake alert level high\n",
    "        return False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We manually create the csv file from the excel and use pandas to read the csv.\n",
    "- For some reason when we use the read_excel function, the number of NaN increase to 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('datasets/input_data.csv')\n",
    "#mydata = pd.read_excel(\"datasets/input_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mydata.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Observation:\n",
    "- There are 8500 records in the dataset\n",
    "- Each Dataset contains 4 columns\n",
    "- The column 'Caller' seems to contain only junk. We will drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = mydata.drop('Caller',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data check #1:\n",
    "mydata.describe(include='all') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also notice some records with junks characters in Short Description and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata['Short description'] = mydata['Short description'].astype(str)\n",
    "mydata['Description'] = mydata['Description'].astype(str)\n",
    "mydata['Assignment group'] = mydata['Assignment group'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset for garbage data\n",
    "mydata[~mydata.iloc[:,:-1].applymap(detect_garbage).all(1)]\n",
    "mydata['Description'].apply(detect_garbage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mydata.iloc[7126]['Short description'])\n",
    "print(mydata.iloc[7969]['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take an example of row# 7126 Short Desc and fix it\n",
    "print('Junk text: \\033[1m%s\\033[0m\\nFixed text: \\033[1m%s\\033[0m' % (mydata['Short description'][7126], \n",
    "                                                                        fix_text(mydata['Short description'][7126])))\n",
    "\n",
    "# List all mojibakes defined in ftfy library\n",
    "print('\\nMojibake Symbol RegEx:\\n', badness.MOJIBAKE_SYMBOL_RE.pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitize the dataset from Mojibakes\n",
    "mydata['Short description'] = mydata['Short description'].apply(fix_text_segment)\n",
    "mydata['Description'] = mydata['Description'].apply(fix_text)\n",
    "\n",
    "# Visualize that row# 7126\n",
    "mydata.iloc[7126,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Observation\n",
    "- There seem to a few invalid values in SHort Description & Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Missing data check #2 : \n",
    "## Are there any null values\n",
    "mydata.isna().apply(pd.value_counts)\n",
    "## Short Description contains 2 nulls and Description contains 1 null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "null_data = mydata[mydata.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method is useful because it shows count, mean, and standard deviation along with the 5 point summary\n",
    "mydata.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of classes in the Assignment Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mydata['Assignment group'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment Group Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata['Assignment group'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Assignment Group Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_assignment_group_dist = mydata['Assignment group'].value_counts().reset_index()\n",
    "df_assignment_group_dist['percentage'] = (df_assignment_group_dist['Assignment group']/df_assignment_group_dist['Assignment group'].sum())*100\n",
    "df_assignment_group_dist.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to visualize the percentage data distribution across different groups\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(20,5))\n",
    "order = mydata[\"Assignment group\"].value_counts().index\n",
    "\n",
    "ax = sns.countplot(x=\"Assignment group\", data=mydata, order=order, linewidth=2,\n",
    "                  edgecolor = \"k\"*len(order), palette='Set1')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "for p in ax.patches:\n",
    "  ax.annotate(str(format(p.get_height()/len(mydata.index)*100, '.2f')+\"%\"), \n",
    "              (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'bottom',\n",
    "              rotation=90, xytext = (0, 10), textcoords = 'offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's cleanup the null values in Short Description and Description fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[mydata['Description'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[mydata['Short description'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace NaN values in Short Description and Description columns\n",
    "mydata['Short description'] = mydata['Short description'].replace(np.nan, '', regex=True)\n",
    "mydata['Description'] = mydata['Description'].replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reconfirmation\n",
    "null_data = mydata[mydata.isnull().any(axis=1)]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "### We will now attempt to remove unwanted text in the 2 columns of interest to us: \n",
    "- Short description\n",
    "- Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for cleaning\n",
    "columnsToPreprocess = ['Short description', 'Description']\n",
    "# Create list of regex to remove sensitive data\n",
    "# Clean dataset and remove sensitive data\n",
    "mydata = preprocess(mydata, columnsToPreprocess, getRegexList())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's merge the Short Description and Description to a new field - Combined Description . This will help us create a rich corpus\n",
    "- Please note we are doing this now to help us with the word cloud step. \n",
    "- We will repeat this step later again if we find non english characters that we need to translate. This step will be repeated after the translation is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging  the 2 preprocessed columns to a single column without duplicate words\n",
    "mydata['Combined description'] = mydata['Short description'] .map(str) + ' ' +  mydata['Description'].map(str)\n",
    "                    \n",
    "mydata['Combined description'] = mydata['Combined description'].apply(lambda x: ' '.join(pd.unique(x.split()))) \n",
    "   \n",
    "#testing on single entry\n",
    "print(mydata.iloc[279]['Short description'])\n",
    "print(mydata.iloc[279]['Description'])\n",
    "print(mydata.iloc[279]['Combined description']) \n",
    "print(mydata.iloc[7126]['Short description'])\n",
    "print(mydata.iloc[7126]['Combined description'])\n",
    "print(mydata.iloc[7969]['Description'])\n",
    "print(mydata.iloc[7969]['Combined description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now let's print the word cloud\n",
    "- Word clouds (also known as text clouds or tag clouds) work in a simple way: the more a specific word appears in a source of al data (such as a speech blog post, or database), the bigger and bolder it appears in the word cloud.\n",
    "\n",
    "- A word cloud is a collection, or cluster, of words depicted in different sizes. The bigger and bolder the word appears, the more often it’s mentioned within a given text and the more important it is.\n",
    "\n",
    "- Also known as tag clouds or text clouds, these are ideal ways to pull out the most pertinent parts of textual data, from blog posts to databases. They can also help business users compare and contrast two different pieces of text to find the wording similarities between the two. \n",
    "\n",
    "#### We will print the word cloud for the top 5 groups - GRP_0, GRP_8, GRP_24, GRP_12, GRP_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = fn_word_cloud(mydata[mydata['Assignment group']=='GRP_0'][\"Combined description\"])\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = fn_word_cloud(mydata[mydata['Assignment group']=='GRP_8'][\"Combined description\"])\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = fn_word_cloud(mydata[mydata['Assignment group']=='GRP_12'][\"Combined description\"])\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = fn_word_cloud(mydata[mydata['Assignment group']=='GRP_9'][\"Combined description\"])\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = fn_word_cloud(mydata[mydata['Assignment group']=='GRP_24'][\"Combined description\"])\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "- Many non english words in GRP_24\n",
    "#### Let us take a quick diversion to look into this further a little bit more\n",
    "- We will first run the google's language detect in multi-threaded fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(20) # Threads\n",
    "\n",
    "def request(text):\n",
    "    #lang = \"zh\"\n",
    "    t = google_translator(timeout=20)\n",
    "#    print(\"Detect Text \" + text)\n",
    "    detect_text = t.detect(text)\n",
    "    #print(detect_text)\n",
    "    return detect_text\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "      time1 = time.time()\n",
    "      #with open(\"datasets/ShortDescriptions.txt\",'r',encoding='utf-8') as f_p:\n",
    "      # texts = f_p.readlines()\n",
    "      #print(texts)\n",
    "      data = mydata['Short description'].values.tolist()\n",
    "      try:\n",
    "          results = pool.map(request, data)\n",
    "          #print(results)\n",
    "      except Exception as e:\n",
    "          raise e\n",
    "      pool.close()\n",
    "      pool.join()\n",
    "\n",
    "      time2 = time.time()\n",
    "      print(\"Detecting %s Short Desciptions, a total of %s s\"%(len(data),time2 - time1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will load the results to a dataframe and pring the last few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame (results,columns=['language', 'language name'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will graph the distribution of languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycol = cycle('bgrcmk')\n",
    "x = df[\"language\"].value_counts()\n",
    "x=x.sort_index()\n",
    "plt.figure(figsize=(10,6))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Distribution of text by language\")\n",
    "plt.ylabel('number of records')\n",
    "plt.xlabel('Language')\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation\n",
    "- Most items are in English followed by German\n",
    "- The other languages are in low single digits - a could in low 2 digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will merge the language columns into the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata = mydata.join(df)\n",
    "mydata.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "- It is interesting to see row 8498 . Short description is in Portugese but Description is in English. \n",
    "- The Combine Description gets interpreted as English (we ran the detect alogrithm separately to confirm this)\n",
    "\n",
    "#### This is the reason we decided to translate Short description and Description independently and then merge them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will attempt Translation into english all the non-english rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(20) # Threads\n",
    "\n",
    "def request(text):\n",
    "    t = google_translator(timeout=20)\n",
    "    translate_text = t.translate(text.strip(), lang_tgt='en')\n",
    "    return translate_text\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "      time1 = time.time()\n",
    "      data = mydata['Short description'].values.tolist()\n",
    "      try:\n",
    "          results = pool.map(request, data)\n",
    "          #print(results)\n",
    "      except Exception as e:\n",
    "          raise e\n",
    "      pool.close()\n",
    "      pool.join()\n",
    "\n",
    "      time2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Translating %s Short Descriptions, a total of %s s\"%(len(data),time2 - time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame (results,columns=['Translated Short description'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will merge the Translated Short description column into the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mydata = mydata.join(df)\n",
    "mydata.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = ThreadPool(20) # Threads\n",
    "\n",
    "def request(text):\n",
    "    t = google_translator(timeout=25)\n",
    "    translate_text = t.translate(text.strip(), lang_tgt='en')\n",
    "    return translate_text\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "      time1 = time.time()\n",
    "      data = mydata['Description'].values.tolist()\n",
    "      try:\n",
    "          results = pool.map(request, data)\n",
    "          #print(results)\n",
    "      except Exception as e:\n",
    "          raise e\n",
    "      pool.close()\n",
    "      pool.join()\n",
    "\n",
    "      time2 = time.time()\n",
    "      print(\"Translating %s Descriptions, a total of %s s\"%(len(data),time2 - time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(\"Translating %s Descriptions, a total of %s s\"%(len(data),time2 - time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame (results,columns=['Translated Description'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will merge the Combined Description column into the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata = mydata.join(df)\n",
    "mydata.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will merge the Short description and Description Columns again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging  the 2 preprocessed columns to a single column without duplicate words\n",
    "mydata['Combined description'] = mydata['Translated Short description'] .map(str) + ' ' +  mydata['Translated Description'].map(str)\n",
    "                    \n",
    "mydata['Combined description'] = mydata['Combined description'].apply(lambda x: ' '.join(pd.unique(x.split()))) \n",
    "   \n",
    "#testing on single entry\n",
    "print(mydata.iloc[279]['Short description'])\n",
    "print(mydata.iloc[279]['Description'])\n",
    "print(mydata.iloc[279]['Combined description']) \n",
    "print(mydata.iloc[7126]['Short description'])\n",
    "print(mydata.iloc[7126]['Combined description'])\n",
    "print(mydata.iloc[7969]['Description'])\n",
    "print(mydata.iloc[7969]['Combined description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mydata.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand contractions\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "# Expanding Contractions in the reviews\n",
    "mydata['Combined description']=mydata['Combined description'].apply(lambda x:expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell =Speller('en')       #Speller(fast=True) for faster but less accurate correctiondata\n",
    "mydata['Combined description']=[' '.join([spell(i) for i in x.split()]) for x in mydata['Combined description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata['CombinedWordCount'] = [len(desc.split(' ')) for desc in mydata['Combined description']]\n",
    "mydata.head()\n",
    "wordCount_before_lemmatization = mydata['CombinedWordCount'].sum()\n",
    "print(\"Total Corpus Word Count before lemmatization: \", wordCount_before_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata['Combined Description Cleaned'] = lemmatize(mydata['Combined description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata['CombinedWordCountCleaned'] = [len(desc.split(' ')) for desc in mydata['Combined Description Cleaned']]\n",
    "wordCount_after_lemmatization = mydata['CombinedWordCountCleaned'].sum()\n",
    "print(\"Total Corpus Word Count after lemmatization: \", wordCount_after_lemmatization)\n",
    "print(\"Max word count of a Document: \", mydata['CombinedWordCountCleaned'].max())\n",
    "print(\"Mean word count of Documents: \", mydata['CombinedWordCountCleaned'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 1 - So far we have performed these steps\n",
    "- Analyzed raw data\n",
    "- Performed Translation \n",
    "- Combined the Short and Long Descriptions into a single field\n",
    "\n",
    "### Now we will run a quick model on how it performs in predicting the group with the data we have.\n",
    "### Then we will explore different data augmentation techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.to_csv('datasets/input_data_after_preprocessing.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Label encode the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "mydata[\"LabelEncodings\"] = le.fit_transform(mydata[\"Assignment group\"])\n",
    "y_classes_len = len(le.classes_)\n",
    "le.classes_\n",
    "print(y_classes_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(mydata['LabelEncodings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(mydata['Combined Description Cleaned'])\n",
    "print(X_train_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(count_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_results = pd.DataFrame(columns=['Algorithm Name', 'Accuracy', 'F1 Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(\"Logistic Regression Score: \", acc_score)\n",
    "f_sc = f1_score(y_test, y_pred, average='weighted', labels=np.unique(y_pred))\n",
    "print(\"Logistic Regression F1 Score: \", f_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
