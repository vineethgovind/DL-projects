{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud\n",
    "#!pip install langdetect\n",
    "#!pip install googletrans\n",
    "#!pip install textblob\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en_core_web_md\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#!pip install langid\n",
    "#!pip install google_trans_new\n",
    "#!pip uninstall googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from langdetect import detect\n",
    "from itertools import cycle\n",
    "#import googletrans\n",
    "#from googletrans import Translator\n",
    "from google_trans_new import google_translator \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "#from textblob import TextBlob\n",
    "#from textblob.translate import NotTranslated\n",
    "import random\n",
    "import operator\n",
    "import math\n",
    "import tqdm\n",
    "import time\n",
    "import spacy\n",
    "import json\n",
    "import langid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section below contains  Useful Functions \n",
    "- As we find new functions, we will create them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = [\"es\", \"de\", \"fr\", \"ar\", \"te\", \"hi\", \"ja\", \"fa\", \"sq\", \"bg\", \"nl\", \"gu\", \"ig\", \"kk\", \"mt\", \"ps\"]\n",
    "\n",
    "def data_augmentation_lang_translation(message, language, aug_range=1):\n",
    "    augmented_messages = []\n",
    "    if hasattr(message, \"decode\"):\n",
    "        message = message.decode(\"utf-8\")\n",
    "\n",
    "        \n",
    "    def request(lang_tgt, text):\n",
    "        t = google_translator(timeout=15)\n",
    "        translate_text = t.translate(text.strip(), lang_tgt)\n",
    "        return translate_text\n",
    "\n",
    "\n",
    "    for j in range(0,aug_range) :\n",
    "            if __name__ == \"__main__\" :\n",
    "              pool = ThreadPool(20) # Threads\n",
    "              time1 = time.time()\n",
    "              try:\n",
    "                  lang_tgt = sr.choice(language)\n",
    "                  func = partial(request, lang_tgt)\n",
    "                  results = pool.map(func, message)\n",
    "              except Exception as e:\n",
    "                  raise e\n",
    "              pool.close()\n",
    "              pool.join()\n",
    "\n",
    "              time2 = time.time()\n",
    "              print(\"Translating %s Descriptions to %s, a total of %s s\"%(\"placeholder\",lang_tgt,time2 - time1))\n",
    "                \n",
    "              pool = ThreadPool(20) # Threads\n",
    "              time1 = time.time()\n",
    "              try:\n",
    "                  lang_tgt = 'en'\n",
    "                  func = partial(request, lang_tgt)\n",
    "                  results = pool.map(func, message)\n",
    "              except Exception as e:\n",
    "                  raise e\n",
    "              pool.close()\n",
    "              pool.join()\n",
    "\n",
    "              time2 = time.time()\n",
    "              #print(\"Translating %s Descriptions, a total of %s s\"%(\"placeholder\",lang_tgt,time2 - time1))\n",
    "\n",
    "              augmented_messages.append(str(results))\n",
    "\n",
    "    return augmented_messages\n",
    "\n",
    "\n",
    "#pool = ThreadPool(20)\n",
    "#def f(a, b, c):\n",
    "#    print(\"{} {} {}\".format(a, b, c))\n",
    "\n",
    "#def main():\n",
    "#    iterable = [1, 2, 3, 4, 5]\n",
    "#    a = \"hi\"\n",
    "#    b = \"there\"\n",
    "#    func = partial(f, a, b)\n",
    "#    pool.map(func, iterable)\n",
    "#    pool.close()\n",
    "#    pool.join()\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Tokenizing sentence into token for finding synonym.\n",
    "def make_tokenizer(texts):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(texts)\n",
    "    return t\n",
    "\n",
    "\n",
    "## Function to find synonym of words \n",
    "spacy.prefer_gpu()\n",
    "#nlp = spacy.load('en', parser=False)\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "def check_lemma(t,w) :\n",
    "    r = [d for d in t if (nlp(d.text)[0].lemma_ != nlp(w.text)[0].lemma_)]\n",
    "    return r\n",
    "\n",
    "def get_word_synonym(word):\n",
    "  filtered_words = [w for w in word.vocab if (not w.lower_ in stop) and w.is_lower == word.is_lower and w.prob >= -20] ## (not w.lower_ in stop) and\n",
    "  similarity = sorted(filtered_words, key=lambda w: word.similarity(w), reverse=True)\n",
    "  filtered_similarity = check_lemma(similarity[:30], word)\n",
    "  return filtered_similarity[:3]\n",
    "\n",
    "## Function for augmenting data by replacing words with synonym using spaCy\n",
    "## This might not be best best method compared to data augmentation using language translation\n",
    "\n",
    "\n",
    "sr_spacy = random.SystemRandom()\n",
    "split_pattern = re.compile(r'\\s+')\n",
    "def data_augmentation_spaCy(message, aug_range=1) :\n",
    "    augmented_messages = []\n",
    "    for j in range(0,aug_range) :\n",
    "        new_message = \"\"\n",
    "        for i in filter(None, split_pattern.split(message)) :\n",
    "            new_message = new_message + \" \" + sr_spacy.choice(filtered_synonym.get(i,[i]))\n",
    "        augmented_messages.append(new_message)\n",
    "    return augmented_messages\n",
    "\n",
    "\n",
    "def loadEmbeddingMatrix(typeToLoad, vocab_dict):\n",
    "    import gensim.models.keyedvectors as word2vec\n",
    "    import gc\n",
    "\n",
    "    # load different embedding file from Kaggle depending on which embedding\n",
    "    # matrix we are going to experiment with\n",
    "    if (typeToLoad == \"gloveTwitter50d\"):\n",
    "        EMBEDDING_FILE = 'embeddings\\glove-twitter-27b-50d/glove.twitter.27B.50d.txt'\n",
    "        embed_size = 50\n",
    "    elif (typeToLoad == \"word2vec\"):\n",
    "        word2vecDict = word2vec.KeyedVectors.load_word2vec_format( \"embeddings\\GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "        embed_size = 300\n",
    "    elif (typeToLoad == \"fasttext\"):\n",
    "        EMBEDDING_FILE = 'embeddings\\\\fasttext/wiki.simple.vec'\n",
    "        embed_size = 300\n",
    "    elif (typeToLoad == \"glove840B300D\"):\n",
    "        EMBEDDING_FILE = 'embeddings\\glove.840B.300d/glove.840B.300d.txt'\n",
    "        embed_size = 300\n",
    "    elif (typeToLoad == \"glove6B300D\"):\n",
    "        EMBEDDING_FILE = 'embeddings\\glove.6B\\glove.6B.300d.txt'\n",
    "        embed_size = 300\n",
    "    elif (typeToLoad == \"paragram\"):\n",
    "        EMBEDDING_FILE = 'embeddings\\paragram_300_sl999\\paragram_300_sl999.txt'\n",
    "        embed_size = 300\n",
    "    elif (typeToLoad == \"wikiNews\"):\n",
    "        EMBEDDING_FILE = \"embeddings\\wiki-news-300d-1M\\wiki-news-300d-1M.vec\"\n",
    "        embed_size = 300\n",
    "\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    if (typeToLoad in [\"gloveTwitter50d\", \"fasttext\"]):\n",
    "        embeddings_index = dict()\n",
    "        # Transfer the embedding weights into a dictionary by iterating through every line of the file.\n",
    "        f = open(EMBEDDING_FILE)\n",
    "        for line in f:\n",
    "            # split up line into an indexed array\n",
    "            values = line.rstrip().rsplit(' ')  # line.split()\n",
    "            # first index is word\n",
    "            word = values[0]\n",
    "            # store the rest of the values in the array as a new array\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs  # 50 dimensions\n",
    "        f.close()\n",
    "    elif (typeToLoad in [\"glove840B300D\", \"paragram\", \"glove6B300D\"]):\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding='latin'))\n",
    "    elif (typeToLoad in [\"wikiNews\"]):\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o) > 100)\n",
    "    else:\n",
    "        embeddings_index = dict()\n",
    "        for word in word2vecDict.wv.vocab:\n",
    "            embeddings_index[word] = word2vecDict.word_vec(word)\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    gc.collect()\n",
    "    # We get the mean and standard deviation of the embedding weights so that we could maintain the\n",
    "    # same statistics for the rest of our own random generated weights.\n",
    "    all_embs = np.stack(list(embeddings_index.values()))\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "    nb_words = len(vocab_dict)\n",
    "    # We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
    "    # the size will be Number of Words in Vocab X Embedding Size\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    gc.collect()\n",
    "\n",
    "    # With the newly created embedding matrix, we'll fill it up with the words that we have in both\n",
    "    # our own dictionary and loaded pretrained embedding.\n",
    "    embeddedCount = 0\n",
    "    for word, i in vocab_dict.items():\n",
    "        #i -= 1\n",
    "        # then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        # and store inside the embedding matrix that we will train later on.\n",
    "        if embedding_vector is not None:\n",
    "            try :\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                embeddedCount += 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "    print('total embedded:', embeddedCount, 'common words')\n",
    "\n",
    "    del embeddings_index\n",
    "    gc.collect()\n",
    "\n",
    "    # finally, return the embedding matrix\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We manually create the csv file from the excell and use pandas to read the csv.\n",
    "- For some reason when we use the read_excel function, the number of NaN increase to 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('datasets/input_data_after_preprocessing.csv')\n",
    "#mydata = pd.read_excel(\"datasets/input_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stop' and don't find synonym of those words.\n",
    "stop = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata['Combined Description Cleaned'] = mydata['Combined Description Cleaned'].astype(str)\n",
    "\n",
    "tokenizer = make_tokenizer(mydata['Combined Description Cleaned'])    \n",
    "\n",
    "#X = tokenizer.texts_to_sequences(mydata['Combined Description Cleaned'])\n",
    "\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "#X = pad_sequences(X, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata['Combined Description Cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary of word index\n",
    "index_word = {}\n",
    "for word in tokenizer.word_index.keys():\n",
    "    index_word[tokenizer.word_index[word]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word list\n",
    "words = [value for key, value in index_word.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Synonym dictionary\n",
    "synonym_dict = {}\n",
    "\n",
    "for word in words:\n",
    "    #if (not check_oos(word)) :\n",
    "        synonym_dict.update({word : tuple([w.lower_ for w in get_word_synonym(nlp.vocab[word])])})\n",
    "        print(word, \" : \", [w.lower_ for w in get_word_synonym(nlp.vocab[word])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "synonym_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only consider filtered synonym\n",
    "import collections\n",
    "value_occurrences = collections.Counter(synonym_dict.values())\n",
    "\n",
    "filtered_synonym = {key  : value for key, value in synonym_dict.items() if value_occurrences[value] == 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = mydata[['Combined Description Cleaned', 'Assignment group']] \n",
    "\n",
    "## Dictionary for 'Combined Description Cleaned' count\n",
    "## 'Combined Description Cleaned' is 'Combined Description Cleaned' name\n",
    "combined_description_count = mydata.value_counts().to_dict()\n",
    "print(combined_description_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get max 'Combined Description Cleaned' count to match other minority classes through data augmentation\n",
    "\n",
    "max_combined_description_count = max(combined_description_count.items(), key=operator.itemgetter(1))[1]\n",
    "print(max_combined_description_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to interate all messages\n",
    "#newdf = pd.DataFrame()\n",
    "#for full_description, count in full_description_count.items() :\n",
    "#    count_diff = max_full_description_count - count    ## Difference to fill\n",
    "#    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "#    if (multiplication_count) :\n",
    "#        old_message_df = pd.DataFrame()\n",
    "#        new_message_df = pd.DataFrame()\n",
    "#        for message in tqdm.tqdm(mydata[mydata[\"Full Description Cleaned\"] == full_description][\"Full Description Cleaned\"]) :\n",
    "#            ## Extracting existing minority class batch\n",
    "#            dummy1 = pd.DataFrame([message], columns=[\"Full Description Cleaned\"])\n",
    "#            dummy1[\"Full Description Cleaned\"] = full_description\n",
    "#            old_message_df = old_message_df.append(dummy1)\n",
    "#            \n",
    "#            ## Creating new augmented batch from existing minority class\n",
    "#            new_messages = data_augmentation_spaCy(message, multiplication_count)\n",
    "#            dummy2 = pd.DataFrame(new_messages, columns=[\"Full Description Cleaned\"])\n",
    "#            dummy2[\"Full Description Cleaned\"] = full_description\n",
    "#            new_message_df = new_message_df.append(dummy2)\n",
    "#        \n",
    "#        ## Select random data points from augmented data\n",
    "#        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "#        \n",
    "#        ## Merge existing and augmented data points\n",
    "#        newdf = newdf.append([old_message_df,new_message_df])\n",
    "#    else :\n",
    "#        newdf = newdf.append(mydata[mydata[\"Full Description Cleaned\"] == full_description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to interate all full descriptions\n",
    "newdf = pd.DataFrame()\n",
    "for item, count in combined_description_count.items() :\n",
    "    combined_description = item[0]\n",
    "    assignment_grp = item[1]\n",
    "    print(combined_description)\n",
    "    print(assignment_grp)\n",
    "    count_diff = max_combined_description_count - count    ## Difference to fill\n",
    "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "    if (multiplication_count) :\n",
    "        old_message_df = pd.DataFrame()\n",
    "        new_message_df = pd.DataFrame()\n",
    "        for message in tqdm.tqdm(mydata[mydata[\"Combined Description Cleaned\"] == combined_description][\"Combined Description Cleaned\"]) :\n",
    "            ## Extracting existing minority class batch\n",
    "            dummy1 = pd.DataFrame(list(zip(message, assignment_grp)), columns=['Combined Description Cleaned', 'Assignment group'])\n",
    "            dummy1[\"Combined Description Cleaned\"] = combined_description\n",
    "            dummy1[\"Assignment group\"] = assignment_grp\n",
    "            old_message_df = old_message_df.append(dummy1)\n",
    "            \n",
    "            ## Creating new augmented batch from existing minority class\n",
    "            new_messages = data_augmentation_lang_translation(message, language, multiplication_count)\n",
    "            dummy2 = pd.DataFrame(list(zip(new_messages, assignment_grp)), columns=['Combined Description Cleaned', 'Assignment group'])\n",
    "            dummy2[\"Combined Description Cleaned\"] = combined_description\n",
    "            dummy2[\"Assignment group\"] = assignment_grp\n",
    "            new_message_df = new_message_df.append(dummy2)\n",
    "        \n",
    "        ## Select random data points from augmented data\n",
    "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "        \n",
    "        ## Merge existing and augmented data points\n",
    "        newdf = newdf.append([old_message_df,new_message_df])\n",
    "    else :\n",
    "        newdf = newdf.append(mydata[mydata[\"Combined Description Cleaned\"] == combined_description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.head(461)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.to_csv('datasets/spaCy_Augmented_Data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/spaCy_synonyms.json', 'w') as fp:\n",
    "    json.dump(synonym_dict, fp, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('datasets/spaCy_filtered_synonyms.json', 'w') as fp:\n",
    "    json.dump(filtered_synonym, fp, sort_keys=True, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
