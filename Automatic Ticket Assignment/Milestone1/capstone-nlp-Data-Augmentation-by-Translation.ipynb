{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud\n",
    "#!pip install langdetect\n",
    "#!pip install googletrans\n",
    "#!pip install textblob\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en_core_web_md\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#!pip install -U spacy-lookups-data\n",
    "#!pip install langid\n",
    "#!pip install google_trans_new\n",
    "#!pip uninstall googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from langdetect import detect\n",
    "from itertools import cycle\n",
    "#import googletrans\n",
    "#from googletrans import Translator\n",
    "from google_trans_new import google_translator \n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "#from textblob import TextBlob\n",
    "#from textblob.translate import NotTranslated\n",
    "import random\n",
    "import operator\n",
    "import math\n",
    "import tqdm\n",
    "import time\n",
    "import spacy\n",
    "import json\n",
    "import langid\n",
    "from bs4 import BeautifulSoup\n",
    "from string import digits\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section below contains  Useful Functions \n",
    "- As we find new functions, we will create them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = [\"es\", \"de\", \"fr\", \"ar\", \"te\", \"hi\", \"ja\", \"fa\", \"sq\", \"bg\", \"nl\", \"gu\", \"ig\", \"kk\", \"mt\", \"ps\"]\n",
    "\n",
    "def data_augmentation_lang_translation(message, language, aug_range=1):\n",
    "    augmented_messages = []\n",
    "    if hasattr(message, \"decode\"):\n",
    "        message = message.decode(\"utf-8\")\n",
    "\n",
    "        \n",
    "    def request(lang_tgt, text):\n",
    "        t = google_translator(timeout=15)\n",
    "        translate_text = t.translate(text.strip(), lang_tgt)\n",
    "        return translate_text\n",
    "\n",
    "\n",
    "    for j in range(0,aug_range) :\n",
    "            if __name__ == \"__main__\" :\n",
    "              pool = ThreadPool(20) # Threads\n",
    "              time1 = time.time()\n",
    "              try:\n",
    "                  lang_tgt = sr.choice(language)\n",
    "                  func = partial(request, lang_tgt)\n",
    "                  results = pool.map(func, message)\n",
    "              except Exception as e:\n",
    "                  raise e\n",
    "              pool.close()\n",
    "              pool.join()\n",
    "\n",
    "              time2 = time.time()\n",
    "              print(\"Translating %s Descriptions to %s, a total of %s s\"%(\"placeholder\",lang_tgt,time2 - time1))\n",
    "                \n",
    "              pool = ThreadPool(20) # Threads\n",
    "              time1 = time.time()\n",
    "              try:\n",
    "                  lang_tgt = 'en'\n",
    "                  func = partial(request, lang_tgt)\n",
    "                  results = pool.map(func, message)\n",
    "              except Exception as e:\n",
    "                  raise e\n",
    "              pool.close()\n",
    "              pool.join()\n",
    "\n",
    "              time2 = time.time()\n",
    "              #print(\"Translating %s Descriptions, a total of %s s\"%(\"placeholder\",lang_tgt,time2 - time1))\n",
    "\n",
    "              augmented_messages.append(str(results))\n",
    "\n",
    "    return augmented_messages\n",
    "\n",
    "\n",
    "#pool = ThreadPool(20)\n",
    "#def f(a, b, c):\n",
    "#    print(\"{} {} {}\".format(a, b, c))\n",
    "\n",
    "#def main():\n",
    "#    iterable = [1, 2, 3, 4, 5]\n",
    "#    a = \"hi\"\n",
    "#    b = \"there\"\n",
    "#    func = partial(f, a, b)\n",
    "#    pool.map(func, iterable)\n",
    "#    pool.close()\n",
    "#    pool.join()\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook we will try out if we can augment data using Translation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('datasets/input_data_after_preprocessing.csv')\n",
    "#mydata = pd.read_excel(\"datasets/input_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation using Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary for intent count\n",
    "## Intent is column name\n",
    "combined_description_count = mydata['Combined Description Cleaned'].value_counts().to_dict()\n",
    "combined_description_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get max intent count to match other minority classes through data augmentation\n",
    "\n",
    "max_combined_description_count = max(combined_description_count.items(), key=operator.itemgetter(1))[1]\n",
    "max_combined_description_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to interate all full descriptions\n",
    "newdf = pd.DataFrame()\n",
    "for combined_description, count in combined_description_count.items() :\n",
    "    count_diff = max_combined_description_count - count    ## Difference to fill\n",
    "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "    if (multiplication_count) :\n",
    "        old_message_df = pd.DataFrame()\n",
    "        new_message_df = pd.DataFrame()\n",
    "        for message in tqdm.tqdm(mydata[mydata[\"Combined Description Cleaned\"] == combined_description][\"Combined Description Cleaned\"]) :\n",
    "            ## Extracting existing minority class batch\n",
    "            dummy1 = pd.DataFrame([message], columns=['Combined Description Cleaned'])\n",
    "            dummy1[\"Combined Description Cleaned\"] = combined_description\n",
    "            old_message_df = old_message_df.append(dummy1)\n",
    "            \n",
    "            ## Creating new augmented batch from existing minority class\n",
    "            new_messages = data_augmentation_lang_translation(message, language, multiplication_count)\n",
    "            dummy2 = pd.DataFrame(new_messages, columns=['Combined Description Cleaned'])\n",
    "            dummy2[\"Combined Description Cleaned\"] = combined_description\n",
    "            new_message_df = new_message_df.append(dummy2)\n",
    "        \n",
    "        ## Select random data points from augmented data\n",
    "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "        \n",
    "        ## Merge existing and augmented data points\n",
    "        newdf = newdf.append([old_message_df,new_message_df])\n",
    "    else :\n",
    "        newdf = newdf.append(mydata[mydata[\"Combined Description Cleaned\"] == combined_description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.head(461)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.to_csv('datasets/output_after_data_augmentation_by_translation.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
