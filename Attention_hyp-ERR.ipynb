{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 0.1  \\\n",
      "0           0             0   \n",
      "1           1             1   \n",
      "2           2             2   \n",
      "3           3             3   \n",
      "4           4             4   \n",
      "\n",
      "                        Combined Description Cleaned Assignment group  \n",
      "0  login issue user manager name checked the name...            GRP_0  \n",
      "1  outlook received from hello team my are not in...            GRP_0  \n",
      "2    cannot log in to received from   cannot on best            GRP_0  \n",
      "3                         unable to access tool page            GRP_0  \n",
      "4                                              error            GRP_0  \n",
      "   Unnamed: 0                       Combined Description Cleaned  \\\n",
      "0           0  login issue user manager name checked the name...   \n",
      "1           1  outlook received from hello team my are not in...   \n",
      "2           2  cannot log in to received from hi i cannot on ...   \n",
      "3           3                         unable to access tool page   \n",
      "4           4                                              error   \n",
      "\n",
      "  Assignment group  LabelEncodings  \n",
      "0            GRP_0               0  \n",
      "1            GRP_0               0  \n",
      "2            GRP_0               0  \n",
      "3            GRP_0               0  \n",
      "4            GRP_0               0  \n",
      "   Unnamed: 0                       Combined Description Cleaned  \\\n",
      "0           0  login issue user manager name checked the name...   \n",
      "1           1  outlook received from hello team my are not in...   \n",
      "2           2  cannot log in to received from hi i cannot on ...   \n",
      "3           3                         unable to access tool page   \n",
      "4           4                                              error   \n",
      "\n",
      "  Assignment group  LabelEncodings  \n",
      "0            GRP_0               0  \n",
      "1            GRP_0               0  \n",
      "2            GRP_0               0  \n",
      "3            GRP_0               0  \n",
      "4            GRP_0               0  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding,LSTM,TimeDistributed,Dense,Flatten,Dropout,RepeatVector,GRU,Bidirectional,Permute\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import kerastuner as kt\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input\n",
    "from keras_preprocessing import sequence\n",
    "from tensorflow.python.keras.layers import concatenate,Concatenate,Activation,Attention, dot\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "filename='datasets/pre_data_dl.csv'\n",
    "data_dl=pd.read_csv(filename)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "print(data_dl.head())\n",
    "filename1='datasets/pre_data_dl_aug1.csv'\n",
    "filename2='datasets/pre_data_dl_aug2.csv'\n",
    "data_dl_aug1=pd.read_csv(filename1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(data_dl_aug1.head())\n",
    "data_dl_aug2=pd.read_csv(filename2)\n",
    "print(data_dl_aug2.head())\n",
    "\n",
    "#filename='pre_data_dl.xlsx'\n",
    "#data_dl=pd.read_excel(filename)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#print(data_dl.head())\n",
    "\n",
    "#filename1='pre_data_dl_aug1.xlsx'\n",
    "#filename2='pre_data_dl_aug2.xlsx'\n",
    "\n",
    "#data_dl_aug1=pd.read_excel(filename1)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#print(data_dl_aug1.head())\n",
    "\n",
    "#data_dl_aug2=pd.read_excel(filename2)\n",
    "#print(data_dl_aug2.head())\n",
    "\n",
    "X = (data_dl[\"Combined Description Cleaned\"])\n",
    "y= (data_dl['Assignment group'])\n",
    "\n",
    "#categorical encoding y\n",
    "y=pd.get_dummies(data_dl['Assignment group'])\n",
    "  \n",
    "\n",
    "\n",
    "#parameters\n",
    "max_features=10000\n",
    "emb_dim=300\n",
    "batch_size=1024\n",
    "epochs=10\n",
    "out_dim=74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size is:  3316\n"
     ]
    }
   ],
   "source": [
    "#function for tokenizer\n",
    "def dfTokenizer(df):\n",
    " tokenizer=Tokenizer(num_words=max_features,char_level=False)\n",
    " tokenizer.fit_on_texts(df)\n",
    " sequences=tokenizer.texts_to_sequences(df)\n",
    " return sequences,tokenizer\n",
    "#tokenization\n",
    "X,tokenizer = dfTokenizer(data_dl[\"Combined Description Cleaned\"]) \n",
    "vocab_size=len(tokenizer.word_index)\n",
    "print(\"vocabulary size is: \",vocab_size)\n",
    "\n",
    "#function for padding\n",
    "def pad(x, length=None):\n",
    " if length is None:\n",
    "   length=max([len(sentence)  for sentence in x])\n",
    " return pad_sequences(x,maxlen=length,padding='post')\n",
    " \n",
    " #padding \n",
    "X=pad_sequences(X,padding='post')\n",
    "\n",
    "\n",
    "#function for splitting the data\n",
    "def split(X,y):\n",
    " X_train_spl,X_test_spl,y_train_spl,y_test_spl=train_test_split(X,y,test_size=0.2,random_state=123)\n",
    " return X_train_spl,X_test_spl,y_train_spl,y_test_spl\n",
    "#split the data\n",
    "X_train,X_test,y_train,y_test=split(X,y)\n",
    "\n",
    "\n",
    "#configuring the callback\n",
    "early_stopping =  EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    patience=3, \n",
    "    min_delta=0.01, \n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "\n",
    "#function for plotting accuracy,loss\n",
    "def plot(model,history):\n",
    " acc = history.history['accuracy']\n",
    " val_acc = history.history['val_accuracy']\n",
    " loss = history.history['loss']\n",
    " val_loss = history.history['val_loss']\n",
    " epochs = range(1, len(acc) + 1)\n",
    " plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    " plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    " plt.title( ' Training and validation accuracy')\n",
    " plt.legend()\n",
    " plt.figure()\n",
    " plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    " plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    " plt.title('Training and validation loss')\n",
    " plt.legend()\n",
    " plt.show()\n",
    " return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attention layer must be called on a list of inputs, namely [query, value] or [query, value, key].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6fd82dba8e6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mforward_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBILSTM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m74\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    958\u001b[0m                                                 input_list)\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1094\u001b[0m           layer=self, inputs=inputs, build_graph=True, training=training_value):\n\u001b[1;32m   1095\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0m\u001b[1;32m   1097\u001b[0m             inputs, input_masks, args, kwargs)\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    826\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    867\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/tensorflow/python/keras/layers/dense_attention.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, return_attention_scores)\u001b[0m\n\u001b[1;32m    145\u001b[0m            \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m            return_attention_scores=False):\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/tensorflow/python/keras/layers/dense_attention.py\u001b[0m in \u001b[0;36m_validate_call_args\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mclass_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m    194\u001b[0m           \u001b[0;34m'{} layer must be called on a list of inputs, namely [query, value] '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           'or [query, value, key].'.format(class_name))\n",
      "\u001b[0;31mValueError\u001b[0m: Attention layer must be called on a list of inputs, namely [query, value] or [query, value, key]."
     ]
    }
   ],
   "source": [
    "inp_len=X.shape[1]\n",
    "inputs = Input(shape=(inp_len,))\n",
    "#attention with encoder only\n",
    "sequence_input = Input(shape=(inp_len), dtype='int32')        \n",
    "embedded_sequences =Embedding(max_features,emb_dim,input_length=inp_len)(sequence_input)     \n",
    "BILSTM= Bidirectional(LSTM(128, return_sequences=True,dropout=0.2,name=\"bi_lstm_0\",return_state=True))(embedded_sequences)\n",
    "#rp=RepeatVector(max_features)(lstm)\n",
    "\n",
    "#xy = tf.reshape(lstm, [189, 256])\n",
    "#rp=RepeatVector(max_features)(xy)\n",
    "\n",
    "BILSTM,forward_h, forward_c, backward_h, backward_c= Bidirectional \\\n",
    "                                                    (LSTM\n",
    "                                                    (128,\n",
    "                                                     dropout=0.2,\n",
    "                                                     return_state=True,\n",
    "                                                    \n",
    "                                                     return_sequences=True))(BILSTM)\n",
    "                                                   \n",
    "                                                   \n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "context_vector,attention_weights = Attention(32)(BILSTM,state_h)\n",
    "fl=Flatten()(context_vector)\n",
    "output = Dense(74, activation='sigmoid')(fl)\n",
    "\n",
    "model_att =tf.keras.Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "print(model_att.summary())\n",
    "\n",
    "\n",
    "#Attention with encoder decoder\n",
    "inp = Embedding(max_features,emb_dim,input_length=inp_len)(inputs)\n",
    "lstm_out = LSTM(64, return_sequences=True)(inp)\n",
    "\n",
    "attention = Dense(1, activation='relu')(lstm_out)\n",
    "attention = Flatten()(attention)\n",
    "attention = Activation('softmax')(attention)\n",
    "attention = RepeatVector(64)(attention)\n",
    "#attention = LSTM(64,return_sequences=False)(attention)\n",
    "attention = Permute([2,1])(attention)\n",
    "\n",
    "combined = concatenate([lstm_out, attention])\n",
    "combined_mul = Flatten()(combined)\n",
    "decode = RepeatVector(64)(combined_mul)\n",
    "\n",
    "decode = LSTM(64, return_sequences=True)(decode)\n",
    "decode=Flatten()(decode)\n",
    "decode = (Dense(74))(decode)\n",
    "\n",
    "decode = Activation('softmax')(decode)\n",
    "\n",
    "model_enc_dec_att = tf.keras.Model(inputs=inputs, outputs=decode)\n",
    "\n",
    "#model= tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[output])\n",
    "model_enc_dec_att.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "print(model_enc_dec_att.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ENC_ATT_model(hp): #attention with encoder\n",
    " model = model_att\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])   \n",
    " model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    " return model\n",
    "\n",
    "def ENC_DEC_ATT_model(hp):#attention with encoder decoder\n",
    " model = model_enc_dec_att\n",
    " print(model.summary())\n",
    " lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])   \n",
    " model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    " return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTuner(kt.tuners.BayesianOptimization):\n",
    "  def run_trial(self, trial, *args, **kwargs):\n",
    "     kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 256, 512,step=256 )\n",
    "     kwargs['epochs'] = trial.hyperparameters.Int('epochs', 5, 10,20)\n",
    "     super(MyTuner, self).run_trial(trial, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sridhar/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-10-d1d94e28a810>\", line 2, in ENC_ATT_model\n",
      "    model = model_att\n",
      "NameError: name 'model_att' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sridhar/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-10-d1d94e28a810>\", line 2, in ENC_ATT_model\n",
      "    model = model_att\n",
      "NameError: name 'model_att' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sridhar/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-10-d1d94e28a810>\", line 2, in ENC_ATT_model\n",
      "    model = model_att\n",
      "NameError: name 'model_att' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sridhar/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-10-d1d94e28a810>\", line 2, in ENC_ATT_model\n",
      "    model = model_att\n",
      "NameError: name 'model_att' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sridhar/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-10-d1d94e28a810>\", line 2, in ENC_ATT_model\n",
      "    model = model_att\n",
      "NameError: name 'model_att' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 0/5\n",
      "Invalid model 1/5\n",
      "Invalid model 2/5\n",
      "Invalid model 3/5\n",
      "Invalid model 4/5\n",
      "Invalid model 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sridhar/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-10-d1d94e28a810>\", line 2, in ENC_ATT_model\n",
      "    model = model_att\n",
      "NameError: name 'model_att' is not defined\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Too many failed attempts to build model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/hypermodel.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, hp)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_distribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d1d94e28a810>\u001b[0m in \u001b[0;36mENC_ATT_model\u001b[0;34m(hp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mENC_ATT_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#attention with encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m  \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_att\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m  \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_att' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-defa669619ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tuner =MyTuner(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mENC_ATT_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mexecutions_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/tuners/bayesian.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hypermodel, objective, max_trials, num_initial_points, alpha, beta, seed, hyperparameters, tune_new_entries, allow_new_entries, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mtune_new_entries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtune_new_entries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             allow_new_entries=allow_new_entries)\n\u001b[0;32m--> 300\u001b[0;31m         super(BayesianOptimization, self, ).__init__(oracle=oracle,\n\u001b[0m\u001b[1;32m    301\u001b[0m                                                      \u001b[0mhypermodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                                                      **kwargs)\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/multi_execution_tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, oracle, hypermodel, executions_per_trial, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m                  \u001b[0mexecutions_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                  **kwargs):\n\u001b[0;32m---> 58\u001b[0;31m         super(MultiExecutionTuner, self).__init__(\n\u001b[0m\u001b[1;32m     59\u001b[0m             oracle, hypermodel, **kwargs)\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 distribution_strategy=distribution_strategy)\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         super(Tuner, self).__init__(oracle=oracle,\n\u001b[0m\u001b[1;32m    101\u001b[0m                                     \u001b[0mhypermodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                                     \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/base_tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, oracle, hypermodel, directory, project_name, logger, overwrite)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_populate_initial_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tuner_fname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/base_tuner.py\u001b[0m in \u001b[0;36m_populate_initial_space\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[1;32m    104\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/hypermodel.py\u001b[0m in \u001b[0;36m_build_wrapper\u001b[0;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# to the search space.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tfnightly_py38/lib/python3.8/site-packages/kerastuner/engine/hypermodel.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, hp)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_fail_streak\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     raise RuntimeError(\n\u001b[0m\u001b[1;32m    113\u001b[0m                         'Too many failed attempts to build model.')\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Too many failed attempts to build model."
     ]
    }
   ],
   "source": [
    "tuner =MyTuner(\n",
    "    ENC_ATT_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/4F\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train,y_train,validation_data=(X_test,y_test))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_ENC_ATT = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for ENC_ATT are\",best_model_ENC_ATT)\n",
    "\n",
    "\n",
    "modelname=\"ENC_ATTModel\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_ENC_ATT.fit(X_train,y_train,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test,y_test),callbacks=[early_stopping])\n",
    "plot(best_model_ENC_ATT,history)\n",
    "\n",
    "scores_ENC_ATT =best_model_ENC_ATT.evaluate(X_test, y_test, verbose=0)\n",
    "scores_ENC_ATT_val = best_model_ENC_ATT.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Accuracy of ENC_ATT for unaugmented data is :\", (scores_ENC_ATT[1]*100))\n",
    "print(\"Validation Accuracy of ENC_ATT for unagumented data is:\", (scores_ENC_ATT_val[1]*100))\n",
    "#print(\"Validation Accuracy of ENC_ATT for unagumented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of ENC_ATT for unaugmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  Training data of ENC_ATT for unaugmented data is :\",np.array(history.history['loss']).mean())\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    ENC_DEC_ATT_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/4G\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train,y_train,validation_data=(X_test,y_test))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_ENC_DEC_ATT = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for ENC_DEC_ATT are\",best_model_ENC_DEC_ATT)\n",
    "\n",
    "\n",
    "modelname=\"ENC_DEC_ATTModel\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_ENC_DEC_ATT.fit(X_train,y_train,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test,y_test),callbacks=[early_stopping])\n",
    "plot(best_model_ENC_DEC_ATT,history)\n",
    "\n",
    "scores_ENC_DEC_ATT =best_model_ENC_DEC_ATT.evaluate(X_test, y_test, verbose=0)\n",
    "scores_ENC_DEC_ATT_val = best_model_ENC_DEC_ATT.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Accuracy of ENC_DEC_ATT for unaugmented data is :\", (scores_ENC_DEC_ATT[1]*100))\n",
    "print(\"Validation Accuracy of ENC_DEC_ATT for unagumented data is:\", (scores_ENC_DEC_ATT_val[1]*100))\n",
    "#print(\"Validation Accuracy of ENC_DEC_ATT for unagumented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of ENC_DEC_ATT for unaugmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  Training data of ENC_DEC_ATT for unaugmented data is :\",np.array(history.history['loss']).mean())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug1 = (data_dl_aug1[\"Combined Description Cleaned\"])\n",
    "y_aug1= (data_dl_aug1['Assignment group'])\n",
    "\n",
    "#categorical encoding y\n",
    "y_aug1=pd.get_dummies(data_dl_aug2['Assignment group'])\n",
    "\n",
    "\n",
    "X_aug2 = (data_dl_aug2[\"Combined Description Cleaned\"])\n",
    "y_aug2= (data_dl_aug2['Assignment group'])\n",
    "\n",
    "#categorical encoding y\n",
    "y_aug2=pd.get_dummies(data_dl_aug2['Assignment group'])\n",
    "\n",
    "#tokenization\n",
    "X_aug1,tokenizer = dfTokenizer(data_dl_aug1[\"Combined Description Cleaned\"]) \n",
    "vocab_size_aug1=len(tokenizer.word_index)\n",
    "print(\"vocabulary size is: \",vocab_size_aug1)\n",
    "\n",
    "X_aug2,tokenizer = dfTokenizer(data_dl_aug2[\"Combined Description Cleaned\"]) \n",
    "vocab_size_aug2=len(tokenizer.word_index)\n",
    "print(\"vocabulary size is: \",vocab_size_aug2)\n",
    "\n",
    "#padding\n",
    "X_aug1=pad_sequences(X_aug1,padding='post')\n",
    "X_aug2=pad_sequences(X_aug2,padding='post')\n",
    "y_aug1=y_aug1[0:17586]\n",
    "\n",
    "def split_stratify(X,y):\n",
    " X_train_spl,X_test_spl,y_train_spl,y_test_spl=train_test_split(X,y,test_size=0.2,stratify=y,random_state=123)\n",
    " return X_train_spl,X_test_spl,y_train_spl,y_test_spl\n",
    "\n",
    "X_train_aug1,X_test_aug1,y_train_aug1,y_test_aug1=split_stratify(X_aug1,y_aug1)\n",
    "X_train_aug2,X_test_aug2,y_train_aug2,y_test_aug2=split_stratify(X_aug2,y_aug2)\n",
    "\n",
    "\n",
    "inp_len1=X_aug1.shape[1]\n",
    "print(inp_len1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(inp_len1), dtype='int32')        \n",
    "embedded_sequences =Embedding(max_features,emb_dim,input_length=inp_len1)(sequence_input)     \n",
    "BILSTM= Bidirectional(LSTM(128, return_sequences=True,dropout=0.2,name=\"bi_lstm_0\",return_state=True))(embedded_sequences)\n",
    "#rp=RepeatVector(max_features)(lstm)\n",
    "\n",
    "#xy = tf.reshape(lstm, [189, 256])\n",
    "#rp=RepeatVector(max_features)(xy)\n",
    "\n",
    "BILSTM,forward_h, forward_c, backward_h, backward_c= Bidirectional \\\n",
    "                                                    (LSTM\n",
    "                                                    (128,\n",
    "                                                     dropout=0.2,\n",
    "                                                     return_state=True,\n",
    "                                                    \n",
    "                                                     return_sequences=True))(BILSTM)\n",
    "                                                   \n",
    "                                                   \n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "context_vector,attention_weights = Attention(32)(BILSTM,state_h)\n",
    "fl=Flatten()(context_vector)\n",
    "output = Dense(74, activation='sigmoid')(fl)\n",
    "\n",
    "model_att1 =tf.keras.Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "print(model_att1.summary())\n",
    "\n",
    "\n",
    "#Attention with encoder decoder\n",
    "inp = Embedding(max_features,emb_dim,input_length=inp_len11)(inputs)\n",
    "lstm_out = LSTM(64, return_sequences=True)(inp)\n",
    "\n",
    "attention = Dense(1, activation='relu')(lstm_out)\n",
    "attention = Flatten()(attention)\n",
    "attention = Activation('softmax')(attention)\n",
    "attention = RepeatVector(64)(attention)\n",
    "#attention = LSTM(64,return_sequences=False)(attention)\n",
    "attention = Permute([2,1])(attention)\n",
    "\n",
    "combined = concatenate([lstm_out, attention])\n",
    "combined_mul = Flatten()(combined)\n",
    "decode = RepeatVector(64)(combined_mul)\n",
    "\n",
    "decode = LSTM(64, return_sequences=True)(decode)\n",
    "decode=Flatten()(decode)\n",
    "decode = (Dense(74))(decode)\n",
    "\n",
    "decode = Activation('softmax')(decode)\n",
    "\n",
    "model_enc_dec_att1 = tf.keras.Model(inputs=inputs, outputs=decode)\n",
    "\n",
    "#model= tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[output])\n",
    "model_enc_dec_att1.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "print(model_enc_dec_att1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    ENC_ATT_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/9A\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug1,y_train_aug1,validation_data=(X_test_aug1,y_test_aug1))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_ENC_ATT = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for ENC_ATT are\",best_model_ENC_ATT)\n",
    "\n",
    "\n",
    "modelname=\"ENC_ATTModel_aug1\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_ENC_ATT.fit(X_train_aug1,y_train_aug1,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug1,y_test_aug1),callbacks=[early_stopping])\n",
    "plot(best_model_ENC_ATT,history)\n",
    "\n",
    "scores_ENC_ATT_aug1 =best_model_ENC_ATT.evaluate(X_test_aug1, y_test_aug1, verbose=0)\n",
    "scores_ENC_ATT_val_aug1 = best_model_ENC_ATT.evaluate(X_train_aug1, y_train_aug1, verbose=0)\n",
    "print(\"Accuracy of ENC_ATT for level1 augmented data is :\", (scores_ENC_ATT_aug1[1]*100))\n",
    "print(\"Validation Accuracy of ENC_ATT for level1 augmented data is:\", (scores_ENC_ATT_val_aug1[1]*100))\n",
    "#print(\"Validation Accuracy of ENC_ATT for level1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of ENC_ATT for level1 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of ENC_ATT for level1 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    ENC_ATT_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/9F\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug2,y_train_aug2,validation_data=(X_test_aug2,y_test_aug2))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_ENC_ATT = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for ENC_ATT are\",best_model_ENC_ATT)\n",
    "\n",
    "\n",
    "modelname=\"ENC_ATTModel_aug2\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_ENC_ATT.fit(X_train_aug2,y_train_aug2,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug2,y_test_aug2),callbacks=[early_stopping])\n",
    "plot(best_model_ENC_ATT,history)\n",
    "\n",
    "scores_ENC_ATT_aug2 =best_model_ENC_ATT.evaluate(X_test_aug2, y_test_aug2, verbose=0)\n",
    "scores_ENC_ATT_val_aug2 = best_model_ENC_ATT.evaluate(X_train_aug2, y_train_aug2, verbose=0)\n",
    "print(\"Accuracy of ENC_ATT for level2 augmented data is :\", (scores_ENC_ATT_aug2[1]*100))\n",
    "print(\"Validation Accuracy of ENC_ATT for level2 augmented data is:\", (scores_ENC_ATT_val_aug2[1]*100))\n",
    "#print(\"Validation Accuracy of ENC_ATT for level1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of ENC_ATT for level2 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of ENC_ATT for level2 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    ENC_DEC_ATT_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/9K\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug1,y_train_aug1,validation_data=(X_test_aug1,y_test_aug1))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_ENC_DEC_ATT = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for ENC_DEC_ATT are\",best_model_ENC_DEC_ATT)\n",
    "\n",
    "\n",
    "modelname=\"ENC_DEC_ATTModel_aug1\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_ENC_DEC_ATT.fit(X_train_aug1,y_train_aug1,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug1,y_test_aug1),callbacks=[early_stopping])\n",
    "plot(best_model_ENC_DEC_ATT,history)\n",
    "\n",
    "scores_ENC_DEC_ATT_aug1 =best_model_ENC_DEC_ATT.evaluate(X_test_aug1, y_test_aug1, verbose=0)\n",
    "scores_ENC_DEC_ATT_val_aug1 = best_model_ENC_DEC_ATT.evaluate(X_train_aug1, y_train_aug1, verbose=0)\n",
    "print(\"Accuracy of ENC_DEC_ATT for level1 augmented data is :\", (scores_ENC_DEC_ATT_aug1[1]*100))\n",
    "print(\"Validation Accuracy of ENC_DEC_ATT for level1 augmented data is:\", (scores_ENC_DEC_ATT_val_aug1[1]*100))\n",
    "#print(\"Validation Accuracy of ENC_DEC_ATT for level1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of ENC_DEC_ATT for level1 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of ENC_DEC_ATT for level1 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner =MyTuner(\n",
    "    ENC_DEC_ATT_model1,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    seed=123,\n",
    "    directory=\"C:/Hyperparameter/9H\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train_aug2,y_train_aug2,validation_data=(X_test_aug2,y_test_aug2))\n",
    "print(tuner.results_summary(5))\n",
    "best_model_ENC_DEC_ATT = tuner.get_best_models()[0]\n",
    "print(\"Hyperparameters for ENC_DEC_ATT are\",best_model_ENC_DEC_ATT)\n",
    "\n",
    "\n",
    "modelname=\"ENC_DEC_ATTModel_aug2\"\n",
    "vals=tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values\n",
    "print(vals)\n",
    "batch_size_hyp=list(vals.values())[1]\n",
    "epochs_hyp=list(vals.values())[2]\n",
    "history=best_model_ENC_DEC_ATT.fit(X_train_aug2,y_train_aug2,batch_size=batch_size_hyp,epochs=epochs_hyp,validation_data=(X_test_aug2,y_test_aug2),callbacks=[early_stopping])\n",
    "plot(best_model_ENC_DEC_ATT,history)\n",
    "\n",
    "scores_ENC_DEC_ATT_aug2 =best_model_ENC_DEC_ATT.evaluate(X_test_aug2, y_test_aug2, verbose=0)\n",
    "scores_ENC_DEC_ATT_val_aug2 = best_model_ENC_DEC_ATT.evaluate(X_train_aug2, y_train_aug2, verbose=0)\n",
    "print(\"Accuracy of ENC_DEC_ATT for level2 augmented data is :\", (scores_ENC_DEC_ATT_aug2[1]*100))\n",
    "print(\"Validation Accuracy of ENC_DEC_ATT for level2 augmented data is:\", (scores_ENC_DEC_ATT_val_aug2[1]*100))\n",
    "#print(\"Validation Accuracy of ENC_DEC_ATT for level1 augmented data is:\" ,np.array(history.history['val_acc']).mean()*100)\n",
    "print( \"Mean loss of  Validation data of ENC_DEC_ATT for level2 augmented data is :\",np.array(history.history['val_loss']).mean())\n",
    "print( \"Mean loss of  training data of ENC_DEC_ATT for level2 augmented data is :\",np.array(history.history['loss']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of Attention with encoder for unaugmented data is :\", (scores_ENC_ATT[1]*100))\n",
    "print(\"Accuracy of Attention with encoder & decoder for unagumented data is:\", (scores_ENC_DEC_ATT[1]*100))\n",
    "print(\"Accuracy of Attention with encoder for level1 augmented data is :\", (scores_ENC_ATT_aug1[1]*100))\n",
    "print(\"Accuracy of Attention with encoder for level2 augmented data is :\", (scores_ENC_ATT_aug2[1]*100))\n",
    "print(\"Accuracy of Attention with encoder & decoder for level1 augmented data is :\", (scores_ENC_DEC_ATT_aug1[1]*100))\n",
    "print(\"Accuracy of Attention with encoder & decoder for level2 augmented data is :\", (scores_ENC_DEC_ATT_aug2[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
